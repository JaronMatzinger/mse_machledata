{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Experiment Management\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "- How experiment management brings observability to ML model development\n",
    "- Workflows for using MLFlow in experiment management, including metric logging, artifact versioning, and hyperparameter optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Management with MLFLow\n",
    "\n",
    "We will be using MLflow Tracking for experiment management. The MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results.\n",
    "\n",
    "There are two important concepts:\n",
    "\n",
    "- **Runs**: Runs are executions of some piece of data science code (e.g. `python train.py`). Each run records metadata (metrics, parameters, start and end times) and as well as the artifacts produced by the code (e.g. model weights).\n",
    "- **Experiments**: An experiment groups together runs for a specific task.\n",
    "\n",
    "![MLFlow concepts](https://mlflow.org/docs/latest/_images/tracking-basics.png)\n",
    "\n",
    "### Launching the MLflow tracking server\n",
    "\n",
    "There are various deployment configurations possible for MLflow. Here we'll simply run it locally, and store everything to local files, but a production setup would usually use cloud storage for artifacts and a database for metadata.\n",
    "\n",
    "![MLflow tracking server setups](https://mlflow.org/docs/latest/_images/tracking-setup-overview.png)\n",
    "\n",
    "To start a local tracking server, run the following in a shell:\n",
    "\n",
    "```shell\n",
    "mlflow server --host 127.0.0.1 --port 8080\n",
    "```\n",
    "\n",
    "### Using the MLflow Client API\n",
    "\n",
    "The `MlflowClient` is one of the primary mechanisms that you will use when training ML models. It enables you to\n",
    "\n",
    "- create new experiments\n",
    "- start runs within experiments\n",
    "- document parameters and metrics for your runs\n",
    "- log artifacts linked to your runs\n",
    "\n",
    "First, import the `MlflowClient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `MlfLowClient` will designate local storage as the tracking server. This means that your experiments, data, models, and everything else you log to MLflow will be stored within the current working directory.\n",
    "\n",
    "To connect to a tracking server, you can set the `tracking_uri` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Default Experiment\n",
    "\n",
    "The Default Experiment is a placeholder that will be used if no explicit experiment is declared. It acts as a fallback for you to ensure that your valuable tracking data is not lost, even if you forget so explicitly create an experiment.\n",
    "\n",
    "Let's see what this default experiment looks like. We can search the available experiments using `MlflowClient.search_experiments()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important!\n",
    "The local mlflow server must be started first:\n",
    "```bash\n",
    "mlflow server --host 127.0.0.1 --port 8080\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = client.search_experiments()\n",
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, `search_experiments` returns a list of `Experiment` objects. `Experiment`s come an ID (`experiment_id`), a storage location for their artifacts (`artifact_location`) and a couple of time stamps - and tags. Tags allow you to attach more information to an experiment. The UI allows you to search for these tags. One \"special\" tag is `mlflow.note.content`, which you can use to attach a note to your experiment.\n",
    "\n",
    "#### Creating an experiment\n",
    "\n",
    "Creating an experiment is straightforward. In the following cell, we demonstrate how to create an experiment with additional metadata attached to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"This is an experiment for a coffee shop to forecast sales.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"coffee-forecasting\",\n",
    "    \"team\": \"stores-ml\",\n",
    "    \"project_quarter\": \"Q1-2024\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "coffee_experiment = client.create_experiment(\n",
    "    name=\"Coffee_Models\", tags=experiment_tags\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the cell above, head over to your MLflow instance. You should see a new experiment in the `Experiments` menu.\n",
    "\n",
    "![image.png](imgs/important_ui_components.png)\n",
    "\n",
    "There are a couple of UI components that are noteworthy here:\n",
    "\n",
    "![image.png](imgs/important_ui_concepts_annotated.png)\n",
    "\n",
    "As you can see, some of the tags we set previously are visible in the UI. Others are not, but they can still be searched using the search mask or the API. You can search experiments using tasks by setting the `filter_string`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/884160673275660681', creation_time=1741858933704, experiment_id='884160673275660681', last_update_time=1741858933704, lifecycle_stage='active', name='Coffee_Models', tags={'mlflow.note.content': 'This is an experiment for a coffee shop to forecast '\n",
       "                         'sales.',\n",
       "  'project_name': 'coffee-forecasting',\n",
       "  'project_quarter': 'Q1-2024',\n",
       "  'team': 'stores-ml'}>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_experiment = client.search_experiments(filter_string=\"tags.`project_name` = 'coffee-forecasting'\")\n",
    "coffee_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are of course better ways of accessing experiments by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/884160673275660681', creation_time=1741858933704, experiment_id='884160673275660681', last_update_time=1741858933704, lifecycle_stage='active', name='Coffee_Models', tags={'mlflow.note.content': 'This is an experiment for a coffee shop to forecast '\n",
       "                        'sales.',\n",
       " 'project_name': 'coffee-forecasting',\n",
       " 'project_quarter': 'Q1-2024',\n",
       " 'team': 'stores-ml'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_experiment = client.get_experiment_by_name(\"Coffee_Models\")\n",
    "coffee_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging to Mlflow\n",
    "\n",
    "In this section we'll be taking a closer look at the core features of MLflow Tracking:\n",
    "- creating new runs using the `start_run` context manager\n",
    "- an introduction to logging\n",
    "- the role of model signatures\n",
    "- logging a trained model\n",
    "\n",
    "#### Keeping track of training\n",
    "\n",
    "As an example, we will be forecasting coffee shop sales (a given, after the previous lab) using machine learning.\n",
    "\n",
    "For our forecasting needs, we will be using [`prophet`](https://facebook.github.io/prophet/). Prophet is a \"forecasting procedure\" developed by Meta. It is fully automated and usually a great start for any time series forecasting project. There's no need to understand the details of Prophet for the purpose of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not importing the `MlflowClient` here. Instead, we will be using the `fluent` API. The fluent API is a globally referenced state of the MLFlow tracking server. This global reference is higher-level API to perform the same actions as the `MlflowClient`.\n",
    "\n",
    "To connect to the MLflow tracking server, simply set the tracking URI as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the experiment, run name and artifact path. If you do not set a run name, MLflow will generate one for you.\n",
    "The artifact path is the path that your model will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_experiment = mlflow.set_experiment(\"Coffee_Models\")\n",
    "run_name = \"coffee_forecast_prophet\"\n",
    "artifact_path = \"coffee_prophet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these definitions out of the way, we can now start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:42:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "10:42:18 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run coffee_forecast_prophet at: http://localhost:8080/#/experiments/884160673275660681/runs/d89d039f7de244fdaa93c4ed7d6f243b\n",
      "üß™ View experiment at: http://localhost:8080/#/experiments/884160673275660681\n"
     ]
    }
   ],
   "source": [
    "# We begin with some data wrangling to prepare the data for Prophet\n",
    "df = pd.read_csv(\"data/coffee_sales.csv\")\n",
    "subset = df[(df[\"product_id\"] == 32) & (df[\"store_id\"] == 8)]\n",
    "\n",
    "# For each day in transaction_date, sum the transaction_qty\n",
    "daily_sales = subset.groupby(\"transaction_date\").agg({\"transaction_qty\": \"sum\"}).reset_index()\n",
    "daily_sales.columns = [\"ds\", \"y\"]\n",
    "daily_sales[\"y\"] = daily_sales[\"y\"].astype(float)\n",
    "\n",
    "# Split the last 30 days of data into a test set\n",
    "train = daily_sales.iloc[:-30]\n",
    "test = daily_sales.iloc[-30:].reset_index()\n",
    "\n",
    "\n",
    "# Define hyperparameters for the Prophet model. Their meaning is not important. \n",
    "# We are just demonstrating how to log hyperparameters\n",
    "params = {\n",
    "    \"seasonality_mode\": \"multiplicative\",\n",
    "    \"changepoint_prior_scale\": 0.05,\n",
    "    \"seasonality_prior_scale\": 10.0,\n",
    "    \"holidays_prior_scale\": 10.0,\n",
    "    \"mcmc_samples\": 0,\n",
    "}\n",
    "\n",
    "# Create a Prophet model and fit it to the training data\n",
    "model = Prophet()\n",
    "model.fit(train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "forecast = model.predict(test)\n",
    "\n",
    "# Compare forecasted values to test set\n",
    "mape = (abs(test[\"y\"] - forecast[\"yhat\"]) / test[\"y\"]).mean()\n",
    "rmse = ((test[\"y\"] - forecast[\"yhat\"]) ** 2).mean() ** 0.5\n",
    "metrics = {\"mape\": mape, \"rmse\": rmse}\n",
    "\n",
    "# Start the MLflow run\n",
    "with mlflow.start_run(run_name=run_name, tags={\"model\": \"Prophet\"}) as run:\n",
    "    # Log the model's hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "    # Log the model's metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    # Log the model itself\n",
    "    mlflow.prophet.log_model(model, artifact_path=artifact_path, input_example=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the previous cell:\n",
    "\n",
    "1. We wrangled some data - nothing new here.\n",
    "2. We created a model using the parameters defined in `params` and fit it to the training data.\n",
    "3. We tested it on a test set and computed some metrics.\n",
    "4. This is where it gets interesting from an MLflow perspective: We created a run using the previously defined `run_name` and then logged the `params`, `metrics`, and the `model` itself to MLflow. When logging a mode, you can pass an example input. This allows MLFlow to infer the signature of your model.\n",
    "\n",
    "Note the `mlflow.prophet.log_model` function: MLflow supports a range of machine learning and deep learning frameworks (they call them [\"model flavors\"](https://mlflow.org/docs/latest/models.html#built-in-model-flavors)). If there is an obscure framework they do are not supporting, you can always log [python functions](https://mlflow.org/docs/latest/models.html#python-function-python-function) and raw files directly. Generally, you can log almost everything to MLflow and they offer dedicated functions for a range of artifacts (e.g. matplotlib `Figure`s, images, numpy data). Refer to the [MLflow docs](https://mlflow.org/docs/latest/python_api/mlflow.html) for a complete list.\n",
    "\n",
    "Your `Coffee_Models` experiment should now look something like the screenshot below.\n",
    "\n",
    "![Coffee_Models with content](imgs/Coffee_Models_with_content.png)\n",
    "\n",
    "You can click on the run to reveal detailed information about the run you logged, including the parameters, metrics, and artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with MLflow\n",
    "\n",
    "So far, we've seen a model that is relatively quick to train. Deep learning models, however, can train for days. We'll now see how MLflow can be used to monitor the training of deep models, similar to tools like tensorboard or weights and biases.\n",
    "\n",
    "As an example, let's (try) solve the [(in)famous XOR-problem](https://en.wikipedia.org/wiki/Perceptron#Universal_approximation_theorem) using a pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "%matplotlib inline\n",
    "torch.manual_seed(2)\n",
    "\n",
    "# Data\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Model\n",
    "class XOR(nn.Module):\n",
    "    def __init__(self, activation=F.sigmoid):\n",
    "        super(XOR, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "# We need room for improvement ;)    \n",
    "activation = nn.Identity()\n",
    "# model = XOR(activation=activation)\n",
    "model = XOR()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, nothing changed. You declare your model and loss function, and select an optimizer.\n",
    "\n",
    "They only place that requires some changes is the training loop. Here, we are going to log the training loss with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100000], Loss: 0.2498592883348465\n",
      "Epoch [100/100000], Loss: 0.2498481124639511\n",
      "Epoch [200/100000], Loss: 0.24983574450016022\n",
      "Epoch [300/100000], Loss: 0.2498220056295395\n",
      "Epoch [400/100000], Loss: 0.24980668723583221\n",
      "Epoch [500/100000], Loss: 0.2497895509004593\n",
      "Epoch [600/100000], Loss: 0.2497703582048416\n",
      "Epoch [700/100000], Loss: 0.24974876642227173\n",
      "Epoch [800/100000], Loss: 0.2497244030237198\n",
      "Epoch [900/100000], Loss: 0.2496967911720276\n",
      "Epoch [1000/100000], Loss: 0.24966539442539215\n",
      "Epoch [1100/100000], Loss: 0.24962957203388214\n",
      "Epoch [1200/100000], Loss: 0.2495884746313095\n",
      "Epoch [1300/100000], Loss: 0.24954111874103546\n",
      "Epoch [1400/100000], Loss: 0.24948638677597046\n",
      "Epoch [1500/100000], Loss: 0.24942269921302795\n",
      "Epoch [1600/100000], Loss: 0.24934834241867065\n",
      "Epoch [1700/100000], Loss: 0.24926093220710754\n",
      "Epoch [1800/100000], Loss: 0.2491576373577118\n",
      "Epoch [1900/100000], Loss: 0.24903492629528046\n",
      "Epoch [2000/100000], Loss: 0.24888822436332703\n",
      "Epoch [2100/100000], Loss: 0.2487117052078247\n",
      "Epoch [2200/100000], Loss: 0.24849823117256165\n",
      "Epoch [2300/100000], Loss: 0.24823828041553497\n",
      "Epoch [2400/100000], Loss: 0.247919961810112\n",
      "Epoch [2500/100000], Loss: 0.2475278675556183\n",
      "Epoch [2600/100000], Loss: 0.24704265594482422\n",
      "Epoch [2700/100000], Loss: 0.24643948674201965\n",
      "Epoch [2800/100000], Loss: 0.24568746984004974\n",
      "Epoch [2900/100000], Loss: 0.24474839866161346\n",
      "Epoch [3000/100000], Loss: 0.2435762882232666\n",
      "Epoch [3100/100000], Loss: 0.2421179860830307\n",
      "Epoch [3200/100000], Loss: 0.2403150200843811\n",
      "Epoch [3300/100000], Loss: 0.23810838162899017\n",
      "Epoch [3400/100000], Loss: 0.23544618487358093\n",
      "Epoch [3500/100000], Loss: 0.23229385912418365\n",
      "Epoch [3600/100000], Loss: 0.2286444902420044\n",
      "Epoch [3700/100000], Loss: 0.22452466189861298\n",
      "Epoch [3800/100000], Loss: 0.21999096870422363\n",
      "Epoch [3900/100000], Loss: 0.2151143103837967\n",
      "Epoch [4000/100000], Loss: 0.20995397865772247\n",
      "Epoch [4100/100000], Loss: 0.20452472567558289\n",
      "Epoch [4200/100000], Loss: 0.19876186549663544\n",
      "Epoch [4300/100000], Loss: 0.1924811601638794\n",
      "Epoch [4400/100000], Loss: 0.1853324919939041\n",
      "Epoch [4500/100000], Loss: 0.17676453292369843\n",
      "Epoch [4600/100000], Loss: 0.16606147587299347\n",
      "Epoch [4700/100000], Loss: 0.1525576263666153\n",
      "Epoch [4800/100000], Loss: 0.13608616590499878\n",
      "Epoch [4900/100000], Loss: 0.11747761815786362\n",
      "Epoch [5000/100000], Loss: 0.09854479134082794\n",
      "Epoch [5100/100000], Loss: 0.08123074471950531\n",
      "Epoch [5200/100000], Loss: 0.06667646765708923\n",
      "Epoch [5300/100000], Loss: 0.0550609715282917\n",
      "Epoch [5400/100000], Loss: 0.04600488394498825\n",
      "Epoch [5500/100000], Loss: 0.03897327184677124\n",
      "Epoch [5600/100000], Loss: 0.0334768146276474\n",
      "Epoch [5700/100000], Loss: 0.02912900783121586\n",
      "Epoch [5800/100000], Loss: 0.02564198523759842\n",
      "Epoch [5900/100000], Loss: 0.022805938497185707\n",
      "Epoch [6000/100000], Loss: 0.020468205213546753\n",
      "Epoch [6100/100000], Loss: 0.018517116084694862\n",
      "Epoch [6200/100000], Loss: 0.016870111227035522\n",
      "Epoch [6300/100000], Loss: 0.015465367585420609\n",
      "Epoch [6400/100000], Loss: 0.014255999587476254\n",
      "Epoch [6500/100000], Loss: 0.013205943629145622\n",
      "Epoch [6600/100000], Loss: 0.012287224642932415\n",
      "Epoch [6700/100000], Loss: 0.011477776803076267\n",
      "Epoch [6800/100000], Loss: 0.010760078206658363\n",
      "Epoch [6900/100000], Loss: 0.010120035149157047\n",
      "Epoch [7000/100000], Loss: 0.009546201676130295\n",
      "Epoch [7100/100000], Loss: 0.009029233828186989\n",
      "Epoch [7200/100000], Loss: 0.008561398833990097\n",
      "Epoch [7300/100000], Loss: 0.008136273361742496\n",
      "Epoch [7400/100000], Loss: 0.007748480886220932\n",
      "Epoch [7500/100000], Loss: 0.007393507286906242\n",
      "Epoch [7600/100000], Loss: 0.00706748990342021\n",
      "Epoch [7700/100000], Loss: 0.006767141167074442\n",
      "Epoch [7800/100000], Loss: 0.006489648018032312\n",
      "Epoch [7900/100000], Loss: 0.006232595071196556\n",
      "Epoch [8000/100000], Loss: 0.005993875674903393\n",
      "Epoch [8100/100000], Loss: 0.005771649070084095\n",
      "Epoch [8200/100000], Loss: 0.005564324092119932\n",
      "Epoch [8300/100000], Loss: 0.005370497237890959\n",
      "Epoch [8400/100000], Loss: 0.005188929382711649\n",
      "Epoch [8500/100000], Loss: 0.005018530413508415\n",
      "Epoch [8600/100000], Loss: 0.004858330823481083\n",
      "Epoch [8700/100000], Loss: 0.004707468673586845\n",
      "Epoch [8800/100000], Loss: 0.00456516956910491\n",
      "Epoch [8900/100000], Loss: 0.004430749919265509\n",
      "Epoch [9000/100000], Loss: 0.004303592722862959\n",
      "Epoch [9100/100000], Loss: 0.004183143377304077\n",
      "Epoch [9200/100000], Loss: 0.004068889655172825\n",
      "Epoch [9300/100000], Loss: 0.003960386384278536\n",
      "Epoch [9400/100000], Loss: 0.003857219126075506\n",
      "Epoch [9500/100000], Loss: 0.003759014420211315\n",
      "Epoch [9600/100000], Loss: 0.003665429539978504\n",
      "Epoch [9700/100000], Loss: 0.0035761583130806684\n",
      "Epoch [9800/100000], Loss: 0.003490916220471263\n",
      "Epoch [9900/100000], Loss: 0.0034094422589987516\n",
      "Epoch [10000/100000], Loss: 0.003331498010084033\n",
      "Epoch [10100/100000], Loss: 0.003256864845752716\n",
      "Epoch [10200/100000], Loss: 0.0031853429973125458\n",
      "Epoch [10300/100000], Loss: 0.003116741543635726\n",
      "Epoch [10400/100000], Loss: 0.0030508986674249172\n",
      "Epoch [10500/100000], Loss: 0.002987646497786045\n",
      "Epoch [10600/100000], Loss: 0.002926839515566826\n",
      "Epoch [10700/100000], Loss: 0.00286834966391325\n",
      "Epoch [10800/100000], Loss: 0.0028120451606810093\n",
      "Epoch [10900/100000], Loss: 0.0027578051667660475\n",
      "Epoch [11000/100000], Loss: 0.0027055246755480766\n",
      "Epoch [11100/100000], Loss: 0.002655100543051958\n",
      "Epoch [11200/100000], Loss: 0.0026064387056976557\n",
      "Epoch [11300/100000], Loss: 0.0025594502221792936\n",
      "Epoch [11400/100000], Loss: 0.00251404894515872\n",
      "Epoch [11500/100000], Loss: 0.0024701664224267006\n",
      "Epoch [11600/100000], Loss: 0.0024277176707983017\n",
      "Epoch [11700/100000], Loss: 0.0023866475094109774\n",
      "Epoch [11800/100000], Loss: 0.0023468786384910345\n",
      "Epoch [11900/100000], Loss: 0.002308361930772662\n",
      "Epoch [12000/100000], Loss: 0.002271032426506281\n",
      "Epoch [12100/100000], Loss: 0.002234840299934149\n",
      "Epoch [12200/100000], Loss: 0.002199738984927535\n",
      "Epoch [12300/100000], Loss: 0.002165677724406123\n",
      "Epoch [12400/100000], Loss: 0.0021326104179024696\n",
      "Epoch [12500/100000], Loss: 0.0021004986483603716\n",
      "Epoch [12600/100000], Loss: 0.00206929724663496\n",
      "Epoch [12700/100000], Loss: 0.0020389712881296873\n",
      "Epoch [12800/100000], Loss: 0.0020094914361834526\n",
      "Epoch [12900/100000], Loss: 0.001980816014111042\n",
      "Epoch [13000/100000], Loss: 0.0019529152195900679\n",
      "Epoch [13100/100000], Loss: 0.0019257565727457404\n",
      "Epoch [13200/100000], Loss: 0.0018993174890056252\n",
      "Epoch [13300/100000], Loss: 0.0018735640915110707\n",
      "Epoch [13400/100000], Loss: 0.0018484725151211023\n",
      "Epoch [13500/100000], Loss: 0.0018240195931866765\n",
      "Epoch [13600/100000], Loss: 0.0018001793650910258\n",
      "Epoch [13700/100000], Loss: 0.001776931923814118\n",
      "Epoch [13800/100000], Loss: 0.0017542573623359203\n",
      "Epoch [13900/100000], Loss: 0.0017321279738098383\n",
      "Epoch [14000/100000], Loss: 0.00171053281519562\n",
      "Epoch [14100/100000], Loss: 0.0016894446453079581\n",
      "Epoch [14200/100000], Loss: 0.0016688565956428647\n",
      "Epoch [14300/100000], Loss: 0.0016487387474626303\n",
      "Epoch [14400/100000], Loss: 0.0016290934290736914\n",
      "Epoch [14500/100000], Loss: 0.0016098851338028908\n",
      "Epoch [14600/100000], Loss: 0.0015911045484244823\n",
      "Epoch [14700/100000], Loss: 0.0015727528370916843\n",
      "Epoch [14800/100000], Loss: 0.0015547963557764888\n",
      "Epoch [14900/100000], Loss: 0.001537232892587781\n",
      "Epoch [15000/100000], Loss: 0.0015200474299490452\n",
      "Epoch [15100/100000], Loss: 0.001503227511420846\n",
      "Epoch [15200/100000], Loss: 0.0014867674326524138\n",
      "Epoch [15300/100000], Loss: 0.0014706465881317854\n",
      "Epoch [15400/100000], Loss: 0.0014548589242622256\n",
      "Epoch [15500/100000], Loss: 0.0014393972232937813\n",
      "Epoch [15600/100000], Loss: 0.0014242492616176605\n",
      "Epoch [15700/100000], Loss: 0.0014094059588387609\n",
      "Epoch [15800/100000], Loss: 0.0013948541600257158\n",
      "Epoch [15900/100000], Loss: 0.0013805932831019163\n",
      "Epoch [16000/100000], Loss: 0.0013666125014424324\n",
      "Epoch [16100/100000], Loss: 0.001352900406345725\n",
      "Epoch [16200/100000], Loss: 0.0013394543202593923\n",
      "Epoch [16300/100000], Loss: 0.0013262589927762747\n",
      "Epoch [16400/100000], Loss: 0.0013133175671100616\n",
      "Epoch [16500/100000], Loss: 0.0013006107183173299\n",
      "Epoch [16600/100000], Loss: 0.001288144732825458\n",
      "Epoch [16700/100000], Loss: 0.0012759063392877579\n",
      "Epoch [16800/100000], Loss: 0.0012638882035389543\n",
      "Epoch [16900/100000], Loss: 0.0012520893942564726\n",
      "Epoch [17000/100000], Loss: 0.001240505836904049\n",
      "Epoch [17100/100000], Loss: 0.0012291223974898458\n",
      "Epoch [17200/100000], Loss: 0.0012179402401670814\n",
      "Epoch [17300/100000], Loss: 0.001206953078508377\n",
      "Epoch [17400/100000], Loss: 0.0011961563723161817\n",
      "Epoch [17500/100000], Loss: 0.0011855444172397256\n",
      "Epoch [17600/100000], Loss: 0.001175113720819354\n",
      "Epoch [17700/100000], Loss: 0.001164856250397861\n",
      "Epoch [17800/100000], Loss: 0.0011547745671123266\n",
      "Epoch [17900/100000], Loss: 0.0011448593577370048\n",
      "Epoch [18000/100000], Loss: 0.0011351098073646426\n",
      "Epoch [18100/100000], Loss: 0.001125515322200954\n",
      "Epoch [18200/100000], Loss: 0.0011160812573507428\n",
      "Epoch [18300/100000], Loss: 0.0011067952727898955\n",
      "Epoch [18400/100000], Loss: 0.0010976573685184121\n",
      "Epoch [18500/100000], Loss: 0.0010886691743507981\n",
      "Epoch [18600/100000], Loss: 0.0010798164876177907\n",
      "Epoch [18700/100000], Loss: 0.001071104547008872\n",
      "Epoch [18800/100000], Loss: 0.0010625332361087203\n",
      "Epoch [18900/100000], Loss: 0.001054083462804556\n",
      "Epoch [19000/100000], Loss: 0.0010457715252414346\n",
      "Epoch [19100/100000], Loss: 0.0010375836864113808\n",
      "Epoch [19200/100000], Loss: 0.0010295136598870158\n",
      "Epoch [19300/100000], Loss: 0.001021579373627901\n",
      "Epoch [19400/100000], Loss: 0.0010137440403923392\n",
      "Epoch [19500/100000], Loss: 0.0010060404893010855\n",
      "Epoch [19600/100000], Loss: 0.0009984293719753623\n",
      "Epoch [19700/100000], Loss: 0.0009909551590681076\n",
      "Epoch [19800/100000], Loss: 0.0009835627861320972\n",
      "Epoch [19900/100000], Loss: 0.0009763019625097513\n",
      "Epoch [20000/100000], Loss: 0.0009691209415905178\n",
      "Epoch [20100/100000], Loss: 0.0009620620985515416\n",
      "Epoch [20200/100000], Loss: 0.0009550802642479539\n",
      "Epoch [20300/100000], Loss: 0.00094822165556252\n",
      "Epoch [20400/100000], Loss: 0.0009414324886165559\n",
      "Epoch [20500/100000], Loss: 0.0009347600862383842\n",
      "Epoch [20600/100000], Loss: 0.000928153342101723\n",
      "Epoch [20700/100000], Loss: 0.0009216676698997617\n",
      "Epoch [20800/100000], Loss: 0.0009152389829978347\n",
      "Epoch [20900/100000], Loss: 0.0009089236846193671\n",
      "Epoch [21000/100000], Loss: 0.0009026714833453298\n",
      "Epoch [21100/100000], Loss: 0.0008965159649960697\n",
      "Epoch [21200/100000], Loss: 0.0008904343121685088\n",
      "Epoch [21300/100000], Loss: 0.0008844301919452846\n",
      "Epoch [21400/100000], Loss: 0.0008785149548202753\n",
      "Epoch [21500/100000], Loss: 0.0008726556552574039\n",
      "Epoch [21600/100000], Loss: 0.0008668969385325909\n",
      "Epoch [21700/100000], Loss: 0.0008611924713477492\n",
      "Epoch [21800/100000], Loss: 0.0008555767708458006\n",
      "Epoch [21900/100000], Loss: 0.0008500201511196792\n",
      "Epoch [22000/100000], Loss: 0.0008445236016996205\n",
      "Epoch [22100/100000], Loss: 0.0008391253650188446\n",
      "Epoch [22200/100000], Loss: 0.0008337676990777254\n",
      "Epoch [22300/100000], Loss: 0.0008284849463962018\n",
      "Epoch [22400/100000], Loss: 0.0008232764666900039\n",
      "Epoch [22500/100000], Loss: 0.0008181037846952677\n",
      "Epoch [22600/100000], Loss: 0.0008130260393954813\n",
      "Epoch [22700/100000], Loss: 0.0008079916005954146\n",
      "Epoch [22800/100000], Loss: 0.0008030094322748482\n",
      "Epoch [22900/100000], Loss: 0.0007981175440363586\n",
      "Epoch [23000/100000], Loss: 0.0007932549342513084\n",
      "Epoch [23100/100000], Loss: 0.000788449018727988\n",
      "Epoch [23200/100000], Loss: 0.0007837259909138083\n",
      "Epoch [23300/100000], Loss: 0.0007790341624058783\n",
      "Epoch [23400/100000], Loss: 0.0007743965252302587\n",
      "Epoch [23500/100000], Loss: 0.0007698287954553962\n",
      "Epoch [23600/100000], Loss: 0.0007653000066056848\n",
      "Epoch [23700/100000], Loss: 0.0007608189480379224\n",
      "Epoch [23800/100000], Loss: 0.0007564091356471181\n",
      "Epoch [23900/100000], Loss: 0.0007520271465182304\n",
      "Epoch [24000/100000], Loss: 0.0007476935279555619\n",
      "Epoch [24100/100000], Loss: 0.0007434347062371671\n",
      "Epoch [24200/100000], Loss: 0.0007392017869278789\n",
      "Epoch [24300/100000], Loss: 0.0007350017549470067\n",
      "Epoch [24400/100000], Loss: 0.000730878789909184\n",
      "Epoch [24500/100000], Loss: 0.0007267949404194951\n",
      "Epoch [24600/100000], Loss: 0.0007227367023006082\n",
      "Epoch [24700/100000], Loss: 0.0007187345763668418\n",
      "Epoch [24800/100000], Loss: 0.0007147802971303463\n",
      "Epoch [24900/100000], Loss: 0.000710858148522675\n",
      "Epoch [25000/100000], Loss: 0.0007069708663038909\n",
      "Epoch [25100/100000], Loss: 0.0007031476707197726\n",
      "Epoch [25200/100000], Loss: 0.0006993556162342429\n",
      "Epoch [25300/100000], Loss: 0.0006955905701033771\n",
      "Epoch [25400/100000], Loss: 0.0006918723229318857\n",
      "Epoch [25500/100000], Loss: 0.0006882028537802398\n",
      "Epoch [25600/100000], Loss: 0.0006845636526122689\n",
      "Epoch [25700/100000], Loss: 0.0006809476763010025\n",
      "Epoch [25800/100000], Loss: 0.0006773893837817013\n",
      "Epoch [25900/100000], Loss: 0.0006738647352904081\n",
      "Epoch [26000/100000], Loss: 0.0006703678518533707\n",
      "Epoch [26100/100000], Loss: 0.0006668961141258478\n",
      "Epoch [26200/100000], Loss: 0.0006634842720814049\n",
      "Epoch [26300/100000], Loss: 0.0006600976921617985\n",
      "Epoch [26400/100000], Loss: 0.000656735384836793\n",
      "Epoch [26500/100000], Loss: 0.0006534000858664513\n",
      "Epoch [26600/100000], Loss: 0.0006501204334199429\n",
      "Epoch [26700/100000], Loss: 0.0006468697101809084\n",
      "Epoch [26800/100000], Loss: 0.0006436369149014354\n",
      "Epoch [26900/100000], Loss: 0.0006404313025996089\n",
      "Epoch [27000/100000], Loss: 0.0006372713833115995\n",
      "Epoch [27100/100000], Loss: 0.0006341449334286153\n",
      "Epoch [27200/100000], Loss: 0.0006310391472652555\n",
      "Epoch [27300/100000], Loss: 0.0006279532099142671\n",
      "Epoch [27400/100000], Loss: 0.0006248980644159019\n",
      "Epoch [27500/100000], Loss: 0.0006219015922397375\n",
      "Epoch [27600/100000], Loss: 0.0006189168780110776\n",
      "Epoch [27700/100000], Loss: 0.0006159486947581172\n",
      "Epoch [27800/100000], Loss: 0.0006129960529506207\n",
      "Epoch [27900/100000], Loss: 0.0006100963219068944\n",
      "Epoch [28000/100000], Loss: 0.0006072302348911762\n",
      "Epoch [28100/100000], Loss: 0.0006043794564902782\n",
      "Epoch [28200/100000], Loss: 0.0006015447434037924\n",
      "Epoch [28300/100000], Loss: 0.0005987245822325349\n",
      "Epoch [28400/100000], Loss: 0.0005959637346677482\n",
      "Epoch [28500/100000], Loss: 0.0005932199419476092\n",
      "Epoch [28600/100000], Loss: 0.0005904958234168589\n",
      "Epoch [28700/100000], Loss: 0.0005877855001017451\n",
      "Epoch [28800/100000], Loss: 0.0005850892048329115\n",
      "Epoch [28900/100000], Loss: 0.000582449953071773\n",
      "Epoch [29000/100000], Loss: 0.0005798314232379198\n",
      "Epoch [29100/100000], Loss: 0.0005772245349362493\n",
      "Epoch [29200/100000], Loss: 0.0005746299284510314\n",
      "Epoch [29300/100000], Loss: 0.0005720507469959557\n",
      "Epoch [29400/100000], Loss: 0.0005695152212865651\n",
      "Epoch [29500/100000], Loss: 0.0005670091486535966\n",
      "Epoch [29600/100000], Loss: 0.000564520072657615\n",
      "Epoch [29700/100000], Loss: 0.0005620429874397814\n",
      "Epoch [29800/100000], Loss: 0.0005595781258307397\n",
      "Epoch [29900/100000], Loss: 0.000557136139832437\n",
      "Epoch [30000/100000], Loss: 0.0005547339096665382\n",
      "Epoch [30100/100000], Loss: 0.0005523465806618333\n",
      "Epoch [30200/100000], Loss: 0.0005499784601852298\n",
      "Epoch [30300/100000], Loss: 0.0005476207006722689\n",
      "Epoch [30400/100000], Loss: 0.0005452778423205018\n",
      "Epoch [30500/100000], Loss: 0.000542965775821358\n",
      "Epoch [30600/100000], Loss: 0.0005406807758845389\n",
      "Epoch [30700/100000], Loss: 0.000538409105502069\n",
      "Epoch [30800/100000], Loss: 0.0005361487856134772\n",
      "Epoch [30900/100000], Loss: 0.0005339020281098783\n",
      "Epoch [31000/100000], Loss: 0.0005316716269589961\n",
      "Epoch [31100/100000], Loss: 0.0005294763832353055\n",
      "Epoch [31200/100000], Loss: 0.0005273021524772048\n",
      "Epoch [31300/100000], Loss: 0.0005251395050436258\n",
      "Epoch [31400/100000], Loss: 0.0005229887319728732\n",
      "Epoch [31500/100000], Loss: 0.0005208510556258261\n",
      "Epoch [31600/100000], Loss: 0.0005187250790186226\n",
      "Epoch [31700/100000], Loss: 0.000516621395945549\n",
      "Epoch [31800/100000], Loss: 0.000514546874910593\n",
      "Epoch [31900/100000], Loss: 0.0005124894087202847\n",
      "Epoch [32000/100000], Loss: 0.0005104403826408088\n",
      "Epoch [32100/100000], Loss: 0.0005084032309241593\n",
      "Epoch [32200/100000], Loss: 0.0005063798162154853\n",
      "Epoch [32300/100000], Loss: 0.0005043714190833271\n",
      "Epoch [32400/100000], Loss: 0.0005023872945457697\n",
      "Epoch [32500/100000], Loss: 0.0005004259292036295\n",
      "Epoch [32600/100000], Loss: 0.0004984723054803908\n",
      "Epoch [32700/100000], Loss: 0.000496525433845818\n",
      "Epoch [32800/100000], Loss: 0.0004945938126184046\n",
      "Epoch [32900/100000], Loss: 0.0004926745314151049\n",
      "Epoch [33000/100000], Loss: 0.0004907731199637055\n",
      "Epoch [33100/100000], Loss: 0.0004888941766694188\n",
      "Epoch [33200/100000], Loss: 0.0004870359261985868\n",
      "Epoch [33300/100000], Loss: 0.00048518431140109897\n",
      "Epoch [33400/100000], Loss: 0.00048334163147956133\n",
      "Epoch [33500/100000], Loss: 0.00048150605289265513\n",
      "Epoch [33600/100000], Loss: 0.0004796837456524372\n",
      "Epoch [33700/100000], Loss: 0.00047787168296054006\n",
      "Epoch [33800/100000], Loss: 0.0004760849988088012\n",
      "Epoch [33900/100000], Loss: 0.0004743195604532957\n",
      "Epoch [34000/100000], Loss: 0.00047256716061383486\n",
      "Epoch [34100/100000], Loss: 0.00047082171658985317\n",
      "Epoch [34200/100000], Loss: 0.00046908174408599734\n",
      "Epoch [34300/100000], Loss: 0.00046735204523429275\n",
      "Epoch [34400/100000], Loss: 0.00046563116484321654\n",
      "Epoch [34500/100000], Loss: 0.0004639264661818743\n",
      "Epoch [34600/100000], Loss: 0.00046223681420087814\n",
      "Epoch [34700/100000], Loss: 0.00046057006693445146\n",
      "Epoch [34800/100000], Loss: 0.0004589112941175699\n",
      "Epoch [34900/100000], Loss: 0.00045726168900728226\n",
      "Epoch [35000/100000], Loss: 0.0004556189524009824\n",
      "Epoch [35100/100000], Loss: 0.00045398736256174743\n",
      "Epoch [35200/100000], Loss: 0.0004523624083958566\n",
      "Epoch [35300/100000], Loss: 0.0004507521807681769\n",
      "Epoch [35400/100000], Loss: 0.000449158251285553\n",
      "Epoch [35500/100000], Loss: 0.00044757931027561426\n",
      "Epoch [35600/100000], Loss: 0.00044601174886338413\n",
      "Epoch [35700/100000], Loss: 0.0004444524529390037\n",
      "Epoch [35800/100000], Loss: 0.000442899065092206\n",
      "Epoch [35900/100000], Loss: 0.00044135417556390166\n",
      "Epoch [36000/100000], Loss: 0.00043981653288938105\n",
      "Epoch [36100/100000], Loss: 0.0004382868646644056\n",
      "Epoch [36200/100000], Loss: 0.00043677669600583613\n",
      "Epoch [36300/100000], Loss: 0.0004352739779278636\n",
      "Epoch [36400/100000], Loss: 0.0004337953869253397\n",
      "Epoch [36500/100000], Loss: 0.0004323210450820625\n",
      "Epoch [36600/100000], Loss: 0.00043085208744741976\n",
      "Epoch [36700/100000], Loss: 0.00042939328704960644\n",
      "Epoch [36800/100000], Loss: 0.0004279415588825941\n",
      "Epoch [36900/100000], Loss: 0.00042649375973269343\n",
      "Epoch [37000/100000], Loss: 0.0004250535275787115\n",
      "Epoch [37100/100000], Loss: 0.0004236316599417478\n",
      "Epoch [37200/100000], Loss: 0.0004222152929287404\n",
      "Epoch [37300/100000], Loss: 0.00042081877472810447\n",
      "Epoch [37400/100000], Loss: 0.0004194291541352868\n",
      "Epoch [37500/100000], Loss: 0.00041804625652730465\n",
      "Epoch [37600/100000], Loss: 0.0004166688013356179\n",
      "Epoch [37700/100000], Loss: 0.0004152992332819849\n",
      "Epoch [37800/100000], Loss: 0.0004139369702897966\n",
      "Epoch [37900/100000], Loss: 0.0004125815466977656\n",
      "Epoch [38000/100000], Loss: 0.0004112395108677447\n",
      "Epoch [38100/100000], Loss: 0.00040990742854774\n",
      "Epoch [38200/100000], Loss: 0.00040857901331037283\n",
      "Epoch [38300/100000], Loss: 0.0004072694282513112\n",
      "Epoch [38400/100000], Loss: 0.0004059635102748871\n",
      "Epoch [38500/100000], Loss: 0.0004046651883982122\n",
      "Epoch [38600/100000], Loss: 0.0004033746663480997\n",
      "Epoch [38700/100000], Loss: 0.0004020913038402796\n",
      "Epoch [38800/100000], Loss: 0.0004008122195955366\n",
      "Epoch [38900/100000], Loss: 0.00039953700616024435\n",
      "Epoch [39000/100000], Loss: 0.00039827474392950535\n",
      "Epoch [39100/100000], Loss: 0.0003970251127611846\n",
      "Epoch [39200/100000], Loss: 0.00039577684947289526\n",
      "Epoch [39300/100000], Loss: 0.0003945420612581074\n",
      "Epoch [39400/100000], Loss: 0.00039332002052105963\n",
      "Epoch [39500/100000], Loss: 0.0003921007155440748\n",
      "Epoch [39600/100000], Loss: 0.00039088702760636806\n",
      "Epoch [39700/100000], Loss: 0.00038968143053352833\n",
      "Epoch [39800/100000], Loss: 0.0003884805191773921\n",
      "Epoch [39900/100000], Loss: 0.0003872820525430143\n",
      "Epoch [40000/100000], Loss: 0.00038608850445598364\n",
      "Epoch [40100/100000], Loss: 0.00038491038139909506\n",
      "Epoch [40200/100000], Loss: 0.00038374189171008766\n",
      "Epoch [40300/100000], Loss: 0.00038257811684161425\n",
      "Epoch [40400/100000], Loss: 0.0003814197552856058\n",
      "Epoch [40500/100000], Loss: 0.000380276411306113\n",
      "Epoch [40600/100000], Loss: 0.00037913647247478366\n",
      "Epoch [40700/100000], Loss: 0.0003779998514801264\n",
      "Epoch [40800/100000], Loss: 0.0003768715832848102\n",
      "Epoch [40900/100000], Loss: 0.00037574482848867774\n",
      "Epoch [41000/100000], Loss: 0.0003746266011148691\n",
      "Epoch [41100/100000], Loss: 0.0003735101781785488\n",
      "Epoch [41200/100000], Loss: 0.00037239911034703255\n",
      "Epoch [41300/100000], Loss: 0.0003713052428793162\n",
      "Epoch [41400/100000], Loss: 0.0003702134417835623\n",
      "Epoch [41500/100000], Loss: 0.0003691239398904145\n",
      "Epoch [41600/100000], Loss: 0.0003680441004689783\n",
      "Epoch [41700/100000], Loss: 0.00036697860923595726\n",
      "Epoch [41800/100000], Loss: 0.0003659164940472692\n",
      "Epoch [41900/100000], Loss: 0.00036485856981016695\n",
      "Epoch [42000/100000], Loss: 0.00036380632082000375\n",
      "Epoch [42100/100000], Loss: 0.00036275704042054713\n",
      "Epoch [42200/100000], Loss: 0.0003617115435190499\n",
      "Epoch [42300/100000], Loss: 0.00036066831671632826\n",
      "Epoch [42400/100000], Loss: 0.000359629892045632\n",
      "Epoch [42500/100000], Loss: 0.0003586059028748423\n",
      "Epoch [42600/100000], Loss: 0.00035758776357397437\n",
      "Epoch [42700/100000], Loss: 0.0003565718652680516\n",
      "Epoch [42800/100000], Loss: 0.00035556015791371465\n",
      "Epoch [42900/100000], Loss: 0.0003545570361893624\n",
      "Epoch [43000/100000], Loss: 0.00035356415901333094\n",
      "Epoch [43100/100000], Loss: 0.00035257748095318675\n",
      "Epoch [43200/100000], Loss: 0.00035159464459866285\n",
      "Epoch [43300/100000], Loss: 0.00035061597009189427\n",
      "Epoch [43400/100000], Loss: 0.0003496405843179673\n",
      "Epoch [43500/100000], Loss: 0.0003486686327960342\n",
      "Epoch [43600/100000], Loss: 0.00034769883495755494\n",
      "Epoch [43700/100000], Loss: 0.000346733519108966\n",
      "Epoch [43800/100000], Loss: 0.0003457776620052755\n",
      "Epoch [43900/100000], Loss: 0.0003448305942583829\n",
      "Epoch [44000/100000], Loss: 0.0003438877174630761\n",
      "Epoch [44100/100000], Loss: 0.00034294702345505357\n",
      "Epoch [44200/100000], Loss: 0.00034200859954580665\n",
      "Epoch [44300/100000], Loss: 0.00034108152613043785\n",
      "Epoch [44400/100000], Loss: 0.00034015937126241624\n",
      "Epoch [44500/100000], Loss: 0.00033924117451533675\n",
      "Epoch [44600/100000], Loss: 0.0003383301373105496\n",
      "Epoch [44700/100000], Loss: 0.00033742052619345486\n",
      "Epoch [44800/100000], Loss: 0.000336514669470489\n",
      "Epoch [44900/100000], Loss: 0.0003356124507263303\n",
      "Epoch [45000/100000], Loss: 0.0003347145102452487\n",
      "Epoch [45100/100000], Loss: 0.0003338181704748422\n",
      "Epoch [45200/100000], Loss: 0.0003329307946842164\n",
      "Epoch [45300/100000], Loss: 0.0003320506657473743\n",
      "Epoch [45400/100000], Loss: 0.00033117260318249464\n",
      "Epoch [45500/100000], Loss: 0.0003303001867607236\n",
      "Epoch [45600/100000], Loss: 0.0003294316993560642\n",
      "Epoch [45700/100000], Loss: 0.0003285661805421114\n",
      "Epoch [45800/100000], Loss: 0.0003277100913692266\n",
      "Epoch [45900/100000], Loss: 0.000326859881170094\n",
      "Epoch [46000/100000], Loss: 0.00032601694692857563\n",
      "Epoch [46100/100000], Loss: 0.0003251740417908877\n",
      "Epoch [46200/100000], Loss: 0.00032433331944048405\n",
      "Epoch [46300/100000], Loss: 0.00032349524553865194\n",
      "Epoch [46400/100000], Loss: 0.0003226591506972909\n",
      "Epoch [46500/100000], Loss: 0.00032182742143049836\n",
      "Epoch [46600/100000], Loss: 0.000320998253300786\n",
      "Epoch [46700/100000], Loss: 0.0003201736544724554\n",
      "Epoch [46800/100000], Loss: 0.00031936090090312064\n",
      "Epoch [46900/100000], Loss: 0.00031854878761805594\n",
      "Epoch [47000/100000], Loss: 0.0003177383332513273\n",
      "Epoch [47100/100000], Loss: 0.00031693081837147474\n",
      "Epoch [47200/100000], Loss: 0.0003161261847708374\n",
      "Epoch [47300/100000], Loss: 0.00031532353023067117\n",
      "Epoch [47400/100000], Loss: 0.0003145346709061414\n",
      "Epoch [47500/100000], Loss: 0.00031375186517834663\n",
      "Epoch [47600/100000], Loss: 0.00031297258101403713\n",
      "Epoch [47700/100000], Loss: 0.0003121953341178596\n",
      "Epoch [47800/100000], Loss: 0.00031142201623879373\n",
      "Epoch [47900/100000], Loss: 0.0003106511430814862\n",
      "Epoch [48000/100000], Loss: 0.0003098818124271929\n",
      "Epoch [48100/100000], Loss: 0.00030911373323760927\n",
      "Epoch [48200/100000], Loss: 0.000308347400277853\n",
      "Epoch [48300/100000], Loss: 0.0003075846179854125\n",
      "Epoch [48400/100000], Loss: 0.0003068311489187181\n",
      "Epoch [48500/100000], Loss: 0.00030608486849814653\n",
      "Epoch [48600/100000], Loss: 0.00030533966491930187\n",
      "Epoch [48700/100000], Loss: 0.00030459603294730186\n",
      "Epoch [48800/100000], Loss: 0.00030385455465875566\n",
      "Epoch [48900/100000], Loss: 0.00030311348382383585\n",
      "Epoch [49000/100000], Loss: 0.0003023766039405018\n",
      "Epoch [49100/100000], Loss: 0.00030164269264787436\n",
      "Epoch [49200/100000], Loss: 0.0003009216452483088\n",
      "Epoch [49300/100000], Loss: 0.0003002026933245361\n",
      "Epoch [49400/100000], Loss: 0.00029948732117190957\n",
      "Epoch [49500/100000], Loss: 0.00029877229826524854\n",
      "Epoch [49600/100000], Loss: 0.000298061640933156\n",
      "Epoch [49700/100000], Loss: 0.0002973499649669975\n",
      "Epoch [49800/100000], Loss: 0.00029664201429113746\n",
      "Epoch [49900/100000], Loss: 0.0002959354897029698\n",
      "Epoch [50000/100000], Loss: 0.0002952313516288996\n",
      "Epoch [50100/100000], Loss: 0.00029452930903062224\n",
      "Epoch [50200/100000], Loss: 0.0002938407997135073\n",
      "Epoch [50300/100000], Loss: 0.0002931544731836766\n",
      "Epoch [50400/100000], Loss: 0.00029246951453387737\n",
      "Epoch [50500/100000], Loss: 0.00029178650584071875\n",
      "Epoch [50600/100000], Loss: 0.0002911053306888789\n",
      "Epoch [50700/100000], Loss: 0.00029042622190900147\n",
      "Epoch [50800/100000], Loss: 0.0002897507802117616\n",
      "Epoch [50900/100000], Loss: 0.00028907693922519684\n",
      "Epoch [51000/100000], Loss: 0.00028840871527791023\n",
      "Epoch [51100/100000], Loss: 0.00028774995007552207\n",
      "Epoch [51200/100000], Loss: 0.00028709290199913085\n",
      "Epoch [51300/100000], Loss: 0.0002864374837372452\n",
      "Epoch [51400/100000], Loss: 0.0002857827639672905\n",
      "Epoch [51500/100000], Loss: 0.0002851304889190942\n",
      "Epoch [51600/100000], Loss: 0.0002844802802428603\n",
      "Epoch [51700/100000], Loss: 0.00028383085737004876\n",
      "Epoch [51800/100000], Loss: 0.0002831870806403458\n",
      "Epoch [51900/100000], Loss: 0.0002825411211233586\n",
      "Epoch [52000/100000], Loss: 0.00028189836302772164\n",
      "Epoch [52100/100000], Loss: 0.000281261105556041\n",
      "Epoch [52200/100000], Loss: 0.000280632491922006\n",
      "Epoch [52300/100000], Loss: 0.0002800031506922096\n",
      "Epoch [52400/100000], Loss: 0.0002793765452224761\n",
      "Epoch [52500/100000], Loss: 0.0002787501725833863\n",
      "Epoch [52600/100000], Loss: 0.00027812496409751475\n",
      "Epoch [52700/100000], Loss: 0.0002775031898636371\n",
      "Epoch [52800/100000], Loss: 0.0002768810954876244\n",
      "Epoch [52900/100000], Loss: 0.00027626188239082694\n",
      "Epoch [53000/100000], Loss: 0.00027564563788473606\n",
      "Epoch [53100/100000], Loss: 0.000275039958069101\n",
      "Epoch [53200/100000], Loss: 0.0002744410594459623\n",
      "Epoch [53300/100000], Loss: 0.000273843266768381\n",
      "Epoch [53400/100000], Loss: 0.0002732473658397794\n",
      "Epoch [53500/100000], Loss: 0.000272651290288195\n",
      "Epoch [53600/100000], Loss: 0.0002720584161579609\n",
      "Epoch [53700/100000], Loss: 0.0002714663278311491\n",
      "Epoch [53800/100000], Loss: 0.00027087549096904695\n",
      "Epoch [53900/100000], Loss: 0.00027028474141843617\n",
      "Epoch [54000/100000], Loss: 0.0002696955925785005\n",
      "Epoch [54100/100000], Loss: 0.0002691090339794755\n",
      "Epoch [54200/100000], Loss: 0.00026852553128264844\n",
      "Epoch [54300/100000], Loss: 0.00026795314624905586\n",
      "Epoch [54400/100000], Loss: 0.0002673827693797648\n",
      "Epoch [54500/100000], Loss: 0.0002668120141606778\n",
      "Epoch [54600/100000], Loss: 0.0002662429178599268\n",
      "Epoch [54700/100000], Loss: 0.0002656752767506987\n",
      "Epoch [54800/100000], Loss: 0.00026511005125939846\n",
      "Epoch [54900/100000], Loss: 0.00026454380713403225\n",
      "Epoch [55000/100000], Loss: 0.0002639798040036112\n",
      "Epoch [55100/100000], Loss: 0.0002634169941302389\n",
      "Epoch [55200/100000], Loss: 0.00026285674539394677\n",
      "Epoch [55300/100000], Loss: 0.0002622982137836516\n",
      "Epoch [55400/100000], Loss: 0.0002617505087982863\n",
      "Epoch [55500/100000], Loss: 0.0002612032985780388\n",
      "Epoch [55600/100000], Loss: 0.00026065786369144917\n",
      "Epoch [55700/100000], Loss: 0.00026011341833509505\n",
      "Epoch [55800/100000], Loss: 0.0002595697878859937\n",
      "Epoch [55900/100000], Loss: 0.0002590291842352599\n",
      "Epoch [56000/100000], Loss: 0.0002584890753496438\n",
      "Epoch [56100/100000], Loss: 0.00025795146939344704\n",
      "Epoch [56200/100000], Loss: 0.00025741499848663807\n",
      "Epoch [56300/100000], Loss: 0.0002568797208368778\n",
      "Epoch [56400/100000], Loss: 0.0002563463640399277\n",
      "Epoch [56500/100000], Loss: 0.00025581486988812685\n",
      "Epoch [56600/100000], Loss: 0.0002552915830165148\n",
      "Epoch [56700/100000], Loss: 0.00025477263261564076\n",
      "Epoch [56800/100000], Loss: 0.00025425435160286725\n",
      "Epoch [56900/100000], Loss: 0.000253736914601177\n",
      "Epoch [57000/100000], Loss: 0.00025322294095531106\n",
      "Epoch [57100/100000], Loss: 0.00025270896730944514\n",
      "Epoch [57200/100000], Loss: 0.00025219711824320257\n",
      "Epoch [57300/100000], Loss: 0.00025168602587655187\n",
      "Epoch [57400/100000], Loss: 0.00025117609766311944\n",
      "Epoch [57500/100000], Loss: 0.0002506674500182271\n",
      "Epoch [57600/100000], Loss: 0.00025016063591465354\n",
      "Epoch [57700/100000], Loss: 0.00024965658667497337\n",
      "Epoch [57800/100000], Loss: 0.0002491532650310546\n",
      "Epoch [57900/100000], Loss: 0.00024865794694051147\n",
      "Epoch [58000/100000], Loss: 0.00024816460791043937\n",
      "Epoch [58100/100000], Loss: 0.0002476731897331774\n",
      "Epoch [58200/100000], Loss: 0.00024718247004784644\n",
      "Epoch [58300/100000], Loss: 0.000246693060034886\n",
      "Epoch [58400/100000], Loss: 0.00024620332987979054\n",
      "Epoch [58500/100000], Loss: 0.00024571490939706564\n",
      "Epoch [58600/100000], Loss: 0.0002452277403790504\n",
      "Epoch [58700/100000], Loss: 0.00024474275414831936\n",
      "Epoch [58800/100000], Loss: 0.0002442570112179965\n",
      "Epoch [58900/100000], Loss: 0.00024377371300943196\n",
      "Epoch [59000/100000], Loss: 0.00024329210282303393\n",
      "Epoch [59100/100000], Loss: 0.00024281349033117294\n",
      "Epoch [59200/100000], Loss: 0.00024234340526163578\n",
      "Epoch [59300/100000], Loss: 0.00024187359667848796\n",
      "Epoch [59400/100000], Loss: 0.00024140487948898226\n",
      "Epoch [59500/100000], Loss: 0.00024093656975310296\n",
      "Epoch [59600/100000], Loss: 0.00024046980252023786\n",
      "Epoch [59700/100000], Loss: 0.0002400032535661012\n",
      "Epoch [59800/100000], Loss: 0.00023953861091285944\n",
      "Epoch [59900/100000], Loss: 0.0002390742301940918\n",
      "Epoch [60000/100000], Loss: 0.0002386118285357952\n",
      "Epoch [60100/100000], Loss: 0.00023815214808564633\n",
      "Epoch [60200/100000], Loss: 0.00023769547988194972\n",
      "Epoch [60300/100000], Loss: 0.00023723937920294702\n",
      "Epoch [60400/100000], Loss: 0.00023678514116909355\n",
      "Epoch [60500/100000], Loss: 0.000236331790802069\n",
      "Epoch [60600/100000], Loss: 0.00023588008480146527\n",
      "Epoch [60700/100000], Loss: 0.00023543828865513206\n",
      "Epoch [60800/100000], Loss: 0.00023499644885305315\n",
      "Epoch [60900/100000], Loss: 0.00023455546761397272\n",
      "Epoch [61000/100000], Loss: 0.0002341159270144999\n",
      "Epoch [61100/100000], Loss: 0.0002336775214644149\n",
      "Epoch [61200/100000], Loss: 0.00023323911591432989\n",
      "Epoch [61300/100000], Loss: 0.00023280234017875046\n",
      "Epoch [61400/100000], Loss: 0.00023236620472744107\n",
      "Epoch [61500/100000], Loss: 0.0002319306368008256\n",
      "Epoch [61600/100000], Loss: 0.0002314961311640218\n",
      "Epoch [61700/100000], Loss: 0.00023106133448891342\n",
      "Epoch [61800/100000], Loss: 0.00023062914260663092\n",
      "Epoch [61900/100000], Loss: 0.0002301959175383672\n",
      "Epoch [62000/100000], Loss: 0.0002297718165209517\n",
      "Epoch [62100/100000], Loss: 0.000229349680012092\n",
      "Epoch [62200/100000], Loss: 0.00022892837296240032\n",
      "Epoch [62300/100000], Loss: 0.00022850626555737108\n",
      "Epoch [62400/100000], Loss: 0.00022808717039879411\n",
      "Epoch [62500/100000], Loss: 0.00022766811889596283\n",
      "Epoch [62600/100000], Loss: 0.00022725010057911277\n",
      "Epoch [62700/100000], Loss: 0.00022683487622998655\n",
      "Epoch [62800/100000], Loss: 0.00022641877876594663\n",
      "Epoch [62900/100000], Loss: 0.00022600484953727573\n",
      "Epoch [63000/100000], Loss: 0.0002255902800243348\n",
      "Epoch [63100/100000], Loss: 0.0002251782570965588\n",
      "Epoch [63200/100000], Loss: 0.00022476825688499957\n",
      "Epoch [63300/100000], Loss: 0.0002243591152364388\n",
      "Epoch [63400/100000], Loss: 0.00022394930419977754\n",
      "Epoch [63500/100000], Loss: 0.0002235412539448589\n",
      "Epoch [63600/100000], Loss: 0.000223133887629956\n",
      "Epoch [63700/100000], Loss: 0.00022273002832662314\n",
      "Epoch [63800/100000], Loss: 0.00022233440540730953\n",
      "Epoch [63900/100000], Loss: 0.00022193719632923603\n",
      "Epoch [64000/100000], Loss: 0.00022154275211505592\n",
      "Epoch [64100/100000], Loss: 0.00022114896273706108\n",
      "Epoch [64200/100000], Loss: 0.0002207554061897099\n",
      "Epoch [64300/100000], Loss: 0.00022036294103600085\n",
      "Epoch [64400/100000], Loss: 0.00021997059229761362\n",
      "Epoch [64500/100000], Loss: 0.00021958212892059237\n",
      "Epoch [64600/100000], Loss: 0.0002191926323575899\n",
      "Epoch [64700/100000], Loss: 0.00021880365966353565\n",
      "Epoch [64800/100000], Loss: 0.00021841638954356313\n",
      "Epoch [64900/100000], Loss: 0.0002180297888116911\n",
      "Epoch [65000/100000], Loss: 0.00021764198027085513\n",
      "Epoch [65100/100000], Loss: 0.00021725872647948563\n",
      "Epoch [65200/100000], Loss: 0.0002168815117329359\n",
      "Epoch [65300/100000], Loss: 0.00021650581038556993\n",
      "Epoch [65400/100000], Loss: 0.0002161292650271207\n",
      "Epoch [65500/100000], Loss: 0.0002157552953576669\n",
      "Epoch [65600/100000], Loss: 0.0002153821405954659\n",
      "Epoch [65700/100000], Loss: 0.00021500902948901057\n",
      "Epoch [65800/100000], Loss: 0.00021463797020260245\n",
      "Epoch [65900/100000], Loss: 0.00021426576131489128\n",
      "Epoch [66000/100000], Loss: 0.00021389499306678772\n",
      "Epoch [66100/100000], Loss: 0.00021352479234337807\n",
      "Epoch [66200/100000], Loss: 0.00021315578487701714\n",
      "Epoch [66300/100000], Loss: 0.00021278744679875672\n",
      "Epoch [66400/100000], Loss: 0.00021242158254608512\n",
      "Epoch [66500/100000], Loss: 0.00021205651864875108\n",
      "Epoch [66600/100000], Loss: 0.00021169104729779065\n",
      "Epoch [66700/100000], Loss: 0.0002113266964443028\n",
      "Epoch [66800/100000], Loss: 0.00021096569253131747\n",
      "Epoch [66900/100000], Loss: 0.0002106040483340621\n",
      "Epoch [67000/100000], Loss: 0.00021024256420787424\n",
      "Epoch [67100/100000], Loss: 0.00020988154574297369\n",
      "Epoch [67200/100000], Loss: 0.00020952484919689596\n",
      "Epoch [67300/100000], Loss: 0.00020917307119816542\n",
      "Epoch [67400/100000], Loss: 0.00020882241369690746\n",
      "Epoch [67500/100000], Loss: 0.00020847131963819265\n",
      "Epoch [67600/100000], Loss: 0.00020812114235013723\n",
      "Epoch [67700/100000], Loss: 0.0002077709068544209\n",
      "Epoch [67800/100000], Loss: 0.0002074221702059731\n",
      "Epoch [67900/100000], Loss: 0.00020707506337203085\n",
      "Epoch [68000/100000], Loss: 0.0002067295426968485\n",
      "Epoch [68100/100000], Loss: 0.00020638410933315754\n",
      "Epoch [68200/100000], Loss: 0.00020603841403499246\n",
      "Epoch [68300/100000], Loss: 0.0002056947269011289\n",
      "Epoch [68400/100000], Loss: 0.00020535039948299527\n",
      "Epoch [68500/100000], Loss: 0.00020500786195043474\n",
      "Epoch [68600/100000], Loss: 0.0002046654699370265\n",
      "Epoch [68700/100000], Loss: 0.00020432747260201722\n",
      "Epoch [68800/100000], Loss: 0.00020399419008754194\n",
      "Epoch [68900/100000], Loss: 0.00020366048556752503\n",
      "Epoch [69000/100000], Loss: 0.0002033272321568802\n",
      "Epoch [69100/100000], Loss: 0.00020299528841860592\n",
      "Epoch [69200/100000], Loss: 0.00020266370847821236\n",
      "Epoch [69300/100000], Loss: 0.00020233163377270103\n",
      "Epoch [69400/100000], Loss: 0.00020200113067403436\n",
      "Epoch [69500/100000], Loss: 0.00020167116599623114\n",
      "Epoch [69600/100000], Loss: 0.00020134189981035888\n",
      "Epoch [69700/100000], Loss: 0.00020101283735129982\n",
      "Epoch [69800/100000], Loss: 0.0002006861031986773\n",
      "Epoch [69900/100000], Loss: 0.00020036037312820554\n",
      "Epoch [70000/100000], Loss: 0.00020003558893222362\n",
      "Epoch [70100/100000], Loss: 0.00019971112487837672\n",
      "Epoch [70200/100000], Loss: 0.00019938712648581713\n",
      "Epoch [70300/100000], Loss: 0.0001990644377656281\n",
      "Epoch [70400/100000], Loss: 0.0001987411524169147\n",
      "Epoch [70500/100000], Loss: 0.00019842019537463784\n",
      "Epoch [70600/100000], Loss: 0.00019809859804809093\n",
      "Epoch [70700/100000], Loss: 0.00019777721900027245\n",
      "Epoch [70800/100000], Loss: 0.00019745762983802706\n",
      "Epoch [70900/100000], Loss: 0.00019713706569746137\n",
      "Epoch [71000/100000], Loss: 0.0001968178548850119\n",
      "Epoch [71100/100000], Loss: 0.00019650073954835534\n",
      "Epoch [71200/100000], Loss: 0.00019618976511992514\n",
      "Epoch [71300/100000], Loss: 0.00019587884889915586\n",
      "Epoch [71400/100000], Loss: 0.00019557063933461905\n",
      "Epoch [71500/100000], Loss: 0.0001952632301254198\n",
      "Epoch [71600/100000], Loss: 0.00019495679589454085\n",
      "Epoch [71700/100000], Loss: 0.00019465028890408576\n",
      "Epoch [71800/100000], Loss: 0.00019434411660768092\n",
      "Epoch [71900/100000], Loss: 0.0001940391812240705\n",
      "Epoch [72000/100000], Loss: 0.00019373410032130778\n",
      "Epoch [72100/100000], Loss: 0.0001934298634296283\n",
      "Epoch [72200/100000], Loss: 0.00019312542281113565\n",
      "Epoch [72300/100000], Loss: 0.00019282236462458968\n",
      "Epoch [72400/100000], Loss: 0.0001925190445035696\n",
      "Epoch [72500/100000], Loss: 0.00019221707771066576\n",
      "Epoch [72600/100000], Loss: 0.00019191413593944162\n",
      "Epoch [72700/100000], Loss: 0.00019161637465003878\n",
      "Epoch [72800/100000], Loss: 0.00019132271700073034\n",
      "Epoch [72900/100000], Loss: 0.00019103038357570767\n",
      "Epoch [73000/100000], Loss: 0.0001907381520140916\n",
      "Epoch [73100/100000], Loss: 0.00019044785585720092\n",
      "Epoch [73200/100000], Loss: 0.00019015654106624424\n",
      "Epoch [73300/100000], Loss: 0.00018986726354341954\n",
      "Epoch [73400/100000], Loss: 0.000189577040146105\n",
      "Epoch [73500/100000], Loss: 0.00018928790814243257\n",
      "Epoch [73600/100000], Loss: 0.00018899877613876015\n",
      "Epoch [73700/100000], Loss: 0.00018871028441935778\n",
      "Epoch [73800/100000], Loss: 0.00018842254939954728\n",
      "Epoch [73900/100000], Loss: 0.00018813461065292358\n",
      "Epoch [74000/100000], Loss: 0.00018784835992846638\n",
      "Epoch [74100/100000], Loss: 0.00018756430654320866\n",
      "Epoch [74200/100000], Loss: 0.00018727932183537632\n",
      "Epoch [74300/100000], Loss: 0.00018699387146625668\n",
      "Epoch [74400/100000], Loss: 0.0001867106038844213\n",
      "Epoch [74500/100000], Loss: 0.0001864268269855529\n",
      "Epoch [74600/100000], Loss: 0.00018614470900502056\n",
      "Epoch [74700/100000], Loss: 0.00018586208170745522\n",
      "Epoch [74800/100000], Loss: 0.00018557935254648328\n",
      "Epoch [74900/100000], Loss: 0.00018529835506342351\n",
      "Epoch [75000/100000], Loss: 0.00018501674639992416\n",
      "Epoch [75100/100000], Loss: 0.0001847358071245253\n",
      "Epoch [75200/100000], Loss: 0.00018445563910063356\n",
      "Epoch [75300/100000], Loss: 0.00018417526734992862\n",
      "Epoch [75400/100000], Loss: 0.00018389664182905108\n",
      "Epoch [75500/100000], Loss: 0.0001836174778873101\n",
      "Epoch [75600/100000], Loss: 0.00018334382912144065\n",
      "Epoch [75700/100000], Loss: 0.00018307230493519455\n",
      "Epoch [75800/100000], Loss: 0.00018280127551406622\n",
      "Epoch [75900/100000], Loss: 0.00018253151210956275\n",
      "Epoch [76000/100000], Loss: 0.00018226253450848162\n",
      "Epoch [76100/100000], Loss: 0.000181993207661435\n",
      "Epoch [76200/100000], Loss: 0.00018172421550843865\n",
      "Epoch [76300/100000], Loss: 0.00018145570356864482\n",
      "Epoch [76400/100000], Loss: 0.0001811883266782388\n",
      "Epoch [76500/100000], Loss: 0.00018092102254740894\n",
      "Epoch [76600/100000], Loss: 0.00018065400945488364\n",
      "Epoch [76700/100000], Loss: 0.00018038714188151062\n",
      "Epoch [76800/100000], Loss: 0.00018012036161962897\n",
      "Epoch [76900/100000], Loss: 0.00017985512386076152\n",
      "Epoch [77000/100000], Loss: 0.00017958910029847175\n",
      "Epoch [77100/100000], Loss: 0.00017932501214090735\n",
      "Epoch [77200/100000], Loss: 0.00017906179709825665\n",
      "Epoch [77300/100000], Loss: 0.00017880454834084958\n",
      "Epoch [77400/100000], Loss: 0.0001785488857422024\n",
      "Epoch [77500/100000], Loss: 0.00017829299031291157\n",
      "Epoch [77600/100000], Loss: 0.00017803696391638368\n",
      "Epoch [77700/100000], Loss: 0.000177781970705837\n",
      "Epoch [77800/100000], Loss: 0.00017752745770849288\n",
      "Epoch [77900/100000], Loss: 0.00017727332306094468\n",
      "Epoch [78000/100000], Loss: 0.00017701921751722693\n",
      "Epoch [78100/100000], Loss: 0.0001767648063832894\n",
      "Epoch [78200/100000], Loss: 0.00017651121015660465\n",
      "Epoch [78300/100000], Loss: 0.00017625882173888385\n",
      "Epoch [78400/100000], Loss: 0.00017600812134332955\n",
      "Epoch [78500/100000], Loss: 0.0001757569843903184\n",
      "Epoch [78600/100000], Loss: 0.00017550868506077677\n",
      "Epoch [78700/100000], Loss: 0.0001752612879499793\n",
      "Epoch [78800/100000], Loss: 0.00017501332331448793\n",
      "Epoch [78900/100000], Loss: 0.00017476679931860417\n",
      "Epoch [79000/100000], Loss: 0.00017451992607675493\n",
      "Epoch [79100/100000], Loss: 0.00017427312559448183\n",
      "Epoch [79200/100000], Loss: 0.00017402804223820567\n",
      "Epoch [79300/100000], Loss: 0.00017378226038999856\n",
      "Epoch [79400/100000], Loss: 0.00017353642033413053\n",
      "Epoch [79500/100000], Loss: 0.00017329234106000513\n",
      "Epoch [79600/100000], Loss: 0.00017304884386248887\n",
      "Epoch [79700/100000], Loss: 0.0001728039642330259\n",
      "Epoch [79800/100000], Loss: 0.00017256017599720508\n",
      "Epoch [79900/100000], Loss: 0.00017231714446097612\n",
      "Epoch [80000/100000], Loss: 0.00017207427299581468\n",
      "Epoch [80100/100000], Loss: 0.00017183198360726237\n",
      "Epoch [80200/100000], Loss: 0.00017158922855742276\n",
      "Epoch [80300/100000], Loss: 0.00017134756490122527\n",
      "Epoch [80400/100000], Loss: 0.00017110607586801052\n",
      "Epoch [80500/100000], Loss: 0.00017086566367652267\n",
      "Epoch [80600/100000], Loss: 0.0001706247276160866\n",
      "Epoch [80700/100000], Loss: 0.00017038827354554087\n",
      "Epoch [80800/100000], Loss: 0.00017015461344271898\n",
      "Epoch [80900/100000], Loss: 0.00016992112796287984\n",
      "Epoch [81000/100000], Loss: 0.0001696876424830407\n",
      "Epoch [81100/100000], Loss: 0.00016945457900874317\n",
      "Epoch [81200/100000], Loss: 0.00016922137001529336\n",
      "Epoch [81300/100000], Loss: 0.00016898789908736944\n",
      "Epoch [81400/100000], Loss: 0.00016875728033483028\n",
      "Epoch [81500/100000], Loss: 0.0001685252063907683\n",
      "Epoch [81600/100000], Loss: 0.00016829399100970477\n",
      "Epoch [81700/100000], Loss: 0.00016806190251372755\n",
      "Epoch [81800/100000], Loss: 0.0001678316039033234\n",
      "Epoch [81900/100000], Loss: 0.0001676008541835472\n",
      "Epoch [82000/100000], Loss: 0.00016736966790631413\n",
      "Epoch [82100/100000], Loss: 0.0001671399804763496\n",
      "Epoch [82200/100000], Loss: 0.00016691125347279012\n",
      "Epoch [82300/100000], Loss: 0.0001666815223870799\n",
      "Epoch [82400/100000], Loss: 0.0001664542651269585\n",
      "Epoch [82500/100000], Loss: 0.00016623154806438833\n",
      "Epoch [82600/100000], Loss: 0.0001660092966631055\n",
      "Epoch [82700/100000], Loss: 0.00016578665236011147\n",
      "Epoch [82800/100000], Loss: 0.0001655653031775728\n",
      "Epoch [82900/100000], Loss: 0.00016534373571630567\n",
      "Epoch [83000/100000], Loss: 0.00016512266302015632\n",
      "Epoch [83100/100000], Loss: 0.00016490239067934453\n",
      "Epoch [83200/100000], Loss: 0.0001646820892347023\n",
      "Epoch [83300/100000], Loss: 0.00016446146764792502\n",
      "Epoch [83400/100000], Loss: 0.00016424170462414622\n",
      "Epoch [83500/100000], Loss: 0.00016402287292294204\n",
      "Epoch [83600/100000], Loss: 0.00016380388115067035\n",
      "Epoch [83700/100000], Loss: 0.00016358616994693875\n",
      "Epoch [83800/100000], Loss: 0.00016336901171598583\n",
      "Epoch [83900/100000], Loss: 0.00016315116954501718\n",
      "Epoch [84000/100000], Loss: 0.00016293642693199217\n",
      "Epoch [84100/100000], Loss: 0.00016272105858661234\n",
      "Epoch [84200/100000], Loss: 0.00016250823682639748\n",
      "Epoch [84300/100000], Loss: 0.0001622936688363552\n",
      "Epoch [84400/100000], Loss: 0.00016208054148592055\n",
      "Epoch [84500/100000], Loss: 0.00016186677385121584\n",
      "Epoch [84600/100000], Loss: 0.00016165441775228828\n",
      "Epoch [84700/100000], Loss: 0.0001614415377844125\n",
      "Epoch [84800/100000], Loss: 0.00016122858505696058\n",
      "Epoch [84900/100000], Loss: 0.00016101638902910054\n",
      "Epoch [85000/100000], Loss: 0.00016080534260254353\n",
      "Epoch [85100/100000], Loss: 0.0001605932047823444\n",
      "Epoch [85200/100000], Loss: 0.00016038191097322851\n",
      "Epoch [85300/100000], Loss: 0.00016017243615351617\n",
      "Epoch [85400/100000], Loss: 0.00015996240836102515\n",
      "Epoch [85500/100000], Loss: 0.00015975293354131281\n",
      "Epoch [85600/100000], Loss: 0.00015954361879266798\n",
      "Epoch [85700/100000], Loss: 0.0001593358174432069\n",
      "Epoch [85800/100000], Loss: 0.00015912622620817274\n",
      "Epoch [85900/100000], Loss: 0.00015891811926849186\n",
      "Epoch [86000/100000], Loss: 0.0001587111910339445\n",
      "Epoch [86100/100000], Loss: 0.00015850305499043316\n",
      "Epoch [86200/100000], Loss: 0.00015829553012736142\n",
      "Epoch [86300/100000], Loss: 0.00015808836906217039\n",
      "Epoch [86400/100000], Loss: 0.00015788218297529966\n",
      "Epoch [86500/100000], Loss: 0.00015767525474075228\n",
      "Epoch [86600/100000], Loss: 0.00015747365250717849\n",
      "Epoch [86700/100000], Loss: 0.000157273854711093\n",
      "Epoch [86800/100000], Loss: 0.0001570729655213654\n",
      "Epoch [86900/100000], Loss: 0.0001568731531733647\n",
      "Epoch [87000/100000], Loss: 0.00015667342813685536\n",
      "Epoch [87100/100000], Loss: 0.00015647441614419222\n",
      "Epoch [87200/100000], Loss: 0.0001562750112498179\n",
      "Epoch [87300/100000], Loss: 0.0001560757082188502\n",
      "Epoch [87400/100000], Loss: 0.000155877904035151\n",
      "Epoch [87500/100000], Loss: 0.000155680303578265\n",
      "Epoch [87600/100000], Loss: 0.00015548302326351404\n",
      "Epoch [87700/100000], Loss: 0.0001552857575006783\n",
      "Epoch [87800/100000], Loss: 0.0001550892775412649\n",
      "Epoch [87900/100000], Loss: 0.00015489327779505402\n",
      "Epoch [88000/100000], Loss: 0.0001546970015624538\n",
      "Epoch [88100/100000], Loss: 0.0001545005798107013\n",
      "Epoch [88200/100000], Loss: 0.00015430522034876049\n",
      "Epoch [88300/100000], Loss: 0.00015411026834044605\n",
      "Epoch [88400/100000], Loss: 0.00015391405031550676\n",
      "Epoch [88500/100000], Loss: 0.00015372253255918622\n",
      "Epoch [88600/100000], Loss: 0.00015353412891272455\n",
      "Epoch [88700/100000], Loss: 0.00015334546333178878\n",
      "Epoch [88800/100000], Loss: 0.00015315601194743067\n",
      "Epoch [88900/100000], Loss: 0.0001529682194814086\n",
      "Epoch [89000/100000], Loss: 0.0001527798449387774\n",
      "Epoch [89100/100000], Loss: 0.0001525921979919076\n",
      "Epoch [89200/100000], Loss: 0.0001524046965641901\n",
      "Epoch [89300/100000], Loss: 0.00015221693320199847\n",
      "Epoch [89400/100000], Loss: 0.00015203004295472056\n",
      "Epoch [89500/100000], Loss: 0.0001518426142865792\n",
      "Epoch [89600/100000], Loss: 0.00015165582590270787\n",
      "Epoch [89700/100000], Loss: 0.00015146932855714113\n",
      "Epoch [89800/100000], Loss: 0.00015128251106943935\n",
      "Epoch [89900/100000], Loss: 0.00015109684318304062\n",
      "Epoch [90000/100000], Loss: 0.00015091126260813326\n",
      "Epoch [90100/100000], Loss: 0.0001507256820332259\n",
      "Epoch [90200/100000], Loss: 0.00015054014511406422\n",
      "Epoch [90300/100000], Loss: 0.0001503567909821868\n",
      "Epoch [90400/100000], Loss: 0.0001501737133366987\n",
      "Epoch [90500/100000], Loss: 0.00014998971892055124\n",
      "Epoch [90600/100000], Loss: 0.0001498071214882657\n",
      "Epoch [90700/100000], Loss: 0.0001496246550232172\n",
      "Epoch [90800/100000], Loss: 0.0001494420284871012\n",
      "Epoch [90900/100000], Loss: 0.00014925983850844204\n",
      "Epoch [91000/100000], Loss: 0.0001490780123276636\n",
      "Epoch [91100/100000], Loss: 0.00014889537123963237\n",
      "Epoch [91200/100000], Loss: 0.00014871408347971737\n",
      "Epoch [91300/100000], Loss: 0.00014853337779641151\n",
      "Epoch [91400/100000], Loss: 0.00014835206093266606\n",
      "Epoch [91500/100000], Loss: 0.00014817090413998812\n",
      "Epoch [91600/100000], Loss: 0.00014799079508520663\n",
      "Epoch [91700/100000], Loss: 0.0001478099584346637\n",
      "Epoch [91800/100000], Loss: 0.000147628816193901\n",
      "Epoch [91900/100000], Loss: 0.0001474494201829657\n",
      "Epoch [92000/100000], Loss: 0.00014726912195328623\n",
      "Epoch [92100/100000], Loss: 0.00014708969683852047\n",
      "Epoch [92200/100000], Loss: 0.00014691086835227907\n",
      "Epoch [92300/100000], Loss: 0.00014673281111754477\n",
      "Epoch [92400/100000], Loss: 0.00014655457925982773\n",
      "Epoch [92500/100000], Loss: 0.00014637669664807618\n",
      "Epoch [92600/100000], Loss: 0.0001461996289435774\n",
      "Epoch [92700/100000], Loss: 0.0001460216735722497\n",
      "Epoch [92800/100000], Loss: 0.00014584508608095348\n",
      "Epoch [92900/100000], Loss: 0.00014566756726708263\n",
      "Epoch [93000/100000], Loss: 0.00014549160550814122\n",
      "Epoch [93100/100000], Loss: 0.000145316356793046\n",
      "Epoch [93200/100000], Loss: 0.00014514109352603555\n",
      "Epoch [93300/100000], Loss: 0.00014496466610580683\n",
      "Epoch [93400/100000], Loss: 0.000144789824844338\n",
      "Epoch [93500/100000], Loss: 0.00014461825776379555\n",
      "Epoch [93600/100000], Loss: 0.00014444893167819828\n",
      "Epoch [93700/100000], Loss: 0.00014427989663090557\n",
      "Epoch [93800/100000], Loss: 0.00014411300071515143\n",
      "Epoch [93900/100000], Loss: 0.00014394412573892623\n",
      "Epoch [94000/100000], Loss: 0.0001437766186427325\n",
      "Epoch [94100/100000], Loss: 0.00014360899513121694\n",
      "Epoch [94200/100000], Loss: 0.00014344269584398717\n",
      "Epoch [94300/100000], Loss: 0.00014327495591714978\n",
      "Epoch [94400/100000], Loss: 0.00014310861297417432\n",
      "Epoch [94500/100000], Loss: 0.0001429419789928943\n",
      "Epoch [94600/100000], Loss: 0.00014277658192440867\n",
      "Epoch [94700/100000], Loss: 0.00014261050091590732\n",
      "Epoch [94800/100000], Loss: 0.00014244383783079684\n",
      "Epoch [94900/100000], Loss: 0.0001422780187567696\n",
      "Epoch [95000/100000], Loss: 0.0001421132474206388\n",
      "Epoch [95100/100000], Loss: 0.00014194783580023795\n",
      "Epoch [95200/100000], Loss: 0.00014178239507600665\n",
      "Epoch [95300/100000], Loss: 0.00014161731814965606\n",
      "Epoch [95400/100000], Loss: 0.00014145232853479683\n",
      "Epoch [95500/100000], Loss: 0.00014128822658676654\n",
      "Epoch [95600/100000], Loss: 0.0001411264092894271\n",
      "Epoch [95700/100000], Loss: 0.0001409675896866247\n",
      "Epoch [95800/100000], Loss: 0.0001408086682204157\n",
      "Epoch [95900/100000], Loss: 0.00014065043069422245\n",
      "Epoch [96000/100000], Loss: 0.00014049134915694594\n",
      "Epoch [96100/100000], Loss: 0.00014033341722097248\n",
      "Epoch [96200/100000], Loss: 0.00014017528155818582\n",
      "Epoch [96300/100000], Loss: 0.00014001759700477123\n",
      "Epoch [96400/100000], Loss: 0.00013986002886667848\n",
      "Epoch [96500/100000], Loss: 0.0001397029700456187\n",
      "Epoch [96600/100000], Loss: 0.00013954458700027317\n",
      "Epoch [96700/100000], Loss: 0.0001393878337694332\n",
      "Epoch [96800/100000], Loss: 0.0001392311096424237\n",
      "Epoch [96900/100000], Loss: 0.00013907421089243144\n",
      "Epoch [97000/100000], Loss: 0.00013891729759052396\n",
      "Epoch [97100/100000], Loss: 0.00013876099546905607\n",
      "Epoch [97200/100000], Loss: 0.00013860441686119884\n",
      "Epoch [97300/100000], Loss: 0.00013844930799677968\n",
      "Epoch [97400/100000], Loss: 0.0001382941409246996\n",
      "Epoch [97500/100000], Loss: 0.0001381401380058378\n",
      "Epoch [97600/100000], Loss: 0.00013798569852951914\n",
      "Epoch [97700/100000], Loss: 0.00013783159374725074\n",
      "Epoch [97800/100000], Loss: 0.0001376783911837265\n",
      "Epoch [97900/100000], Loss: 0.00013752302038483322\n",
      "Epoch [98000/100000], Loss: 0.00013736984692513943\n",
      "Epoch [98100/100000], Loss: 0.00013721657160203904\n",
      "Epoch [98200/100000], Loss: 0.00013706303434446454\n",
      "Epoch [98300/100000], Loss: 0.00013690974446944892\n",
      "Epoch [98400/100000], Loss: 0.00013675731315743178\n",
      "Epoch [98500/100000], Loss: 0.0001366042997688055\n",
      "Epoch [98600/100000], Loss: 0.0001364520430797711\n",
      "Epoch [98700/100000], Loss: 0.0001362991752102971\n",
      "Epoch [98800/100000], Loss: 0.00013614678755402565\n",
      "Epoch [98900/100000], Loss: 0.0001359945599688217\n",
      "Epoch [99000/100000], Loss: 0.00013584323460236192\n",
      "Epoch [99100/100000], Loss: 0.00013569206930696964\n",
      "Epoch [99200/100000], Loss: 0.0001355407148366794\n",
      "Epoch [99300/100000], Loss: 0.0001353901461698115\n",
      "Epoch [99400/100000], Loss: 0.0001352395920548588\n",
      "Epoch [99500/100000], Loss: 0.00013509002747014165\n",
      "Epoch [99600/100000], Loss: 0.00013493910955730826\n",
      "Epoch [99700/100000], Loss: 0.00013478941400535405\n",
      "Epoch [99800/100000], Loss: 0.0001346386707155034\n",
      "Epoch [99900/100000], Loss: 0.0001344900083495304\n",
      "üèÉ View run carefree-ram-498 at: http://localhost:8080/#/experiments/676385774884056881/runs/6f92a0a90208470f91573f88306ee2c0\n",
      "üß™ View experiment at: http://localhost:8080/#/experiments/676385774884056881\n"
     ]
    }
   ],
   "source": [
    "# Implicitly create a new experiment\n",
    "mlflow.set_experiment(\"XOR\")\n",
    "\n",
    "epochs = 100000\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the hyperparameters\n",
    "    # Hyperparameters\n",
    "    hp = {\n",
    "        \"activation\": activation.__class__.__name__,\n",
    "        \"lr\": 0.02,\n",
    "        \"momentum\": 0.9,\n",
    "        \"epochs\": epochs,\n",
    "        \"loss_fn\": loss_fn.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "    }\n",
    "    mlflow.log_params(hp)\n",
    "    # Train the model\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item()}\")\n",
    "            mlflow.log_metric(\"loss\", f\"{loss:2f}\", step=epoch)\n",
    "\n",
    "    # Save the trained model to MLflow.\n",
    "    mlflow.pytorch.log_model(model, \"model\", input_example=X.to(\"cpu\").numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autologging\n",
    "\n",
    "That was easy! It could get tedious, however, when validation and test sets get introduced. Also, we have to manually call all the logging functions everytime we want to save more data to MLflow.\n",
    "\n",
    "Luckily, MLflow comes with `autologging`! Instead of adding all the calls yourself, simply call `mlflow.autolog` any time before `mlflow.start_run`! Make sure to checkout MLflow's guide on [\"Automatic Logging with MLflow Tracking\"](https://mlflow.org/docs/latest/tracking/autolog.html) if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs\n",
    "\n",
    "When we look at our previous attempt at solving the XOR problem, we have to admit that we were not particularly successful.\n",
    "\n",
    "Let's analyze the loss curve. In the UI, select the XOR experiment and then click on the `Chart` tab.\n",
    "\n",
    "![XOR identity chart](imgs/xor_identity_chart.png)\n",
    "It looks like the model hasn't learnt much. Let's swap out the `nn.Identity` for an `F.sigmoid` and rerun the training.\n",
    "After refreshing the page, there should now be an additional run.\n",
    "\n",
    "![XOR sigmoid chart](imgs/xor_sigmoid_chart.png)\n",
    "\n",
    "It looks like the model is finally starting to learn something after step 9000. Maybe it needs more iterations? Increase the the number of epochs to `100000`.\n",
    "\n",
    "![XOR sigmoid chart with more iterations](imgs/xor_sigmoid_chart_with_more_iterations.png)\n",
    "\n",
    "Ah! This looks much better!\n",
    "\n",
    "---\n",
    "\n",
    "There are many more features to the chart view, which we invite you to explore on your own.\n",
    "MLflow's comparison features really begin to shine when it comes to hyperparameter tuning. In the second part of this lab, you will be introduced to a state-of-the-art hyperparameter tuning package and get to play a game of _guess the hyperparameter_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-lab-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
