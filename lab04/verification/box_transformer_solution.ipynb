{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Transformer\n",
    "\n",
    "In this notebook, you are going to implement a very simple box propagation scheme with the goal of verifying the neural network shown below.\n",
    "\n",
    "![Simple Net](../imgs/box_network.png)\n",
    "\n",
    "Weights on the edges are weights. So if there's an edge connecting $a$ and $b$ with weight $2$, this means that $a = 2b$.\n",
    "\n",
    "If multiple edges terminate at a single node, the edges are added, so if both $a$ and $b$ terminate at $c$, this means that $c = a + b$.\n",
    "\n",
    "Finally, numbers next to nodes denote biases. For instance, if $a$ is connected to $b$ and there's a $3$ next to it, this means $b = a + 3$.\n",
    "\n",
    "Remember that $\\operatorname{ReLU}(x) = \\max(0, x)$.\n",
    "\n",
    "In this exercise, we want to prove that $o_5 > o_6$! So, after the box has been propagated through the network,\n",
    "we want all possible values of $o_5$ to be bigger than all possible values of $o_6$. \n",
    "\n",
    "We've implemented all the \"layers\" of this network for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AddLayer(nn.Module):\n",
    "    def __init__(self, bias=0.0):\n",
    "        super(AddLayer, self).__init__()\n",
    "        self.bias = nn.Parameter(torch.tensor(bias))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sum(x, dim=1) + self.bias\n",
    "    \n",
    "class ScalarMulLayer(nn.Module):\n",
    "    def __init__(self, weights=1.0):\n",
    "        super(ScalarMulLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.tensor(weights))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weights * x\n",
    "    \n",
    "class ReLULayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLULayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.max(x, torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a warm-up, use the layers from above to implement the network shown in the image.\n",
    "Note that the layers are `Modules` so you can use `nn.Sequential`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/25l_wmg111g_p_bn38297hgh0000gr/T/ipykernel_24215/2997928356.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.weights = nn.Parameter(torch.tensor(weights))\n",
      "/var/folders/ds/25l_wmg111g_p_bn38297hgh0000gr/T/ipykernel_24215/2997928356.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.bias = nn.Parameter(torch.tensor(bias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7000, -0.3000], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Your code goes here :)\n",
    "model = nn.Sequential(\n",
    "        ScalarMulLayer(\n",
    "            torch.tensor([\n",
    "                [1.0, 1.0],\n",
    "                [1.0, -1.0]\n",
    "            ])\n",
    "        ),\n",
    "        AddLayer(\n",
    "            torch.tensor([0.0, 0.0])\n",
    "        ),\n",
    "        ReLULayer(),\n",
    "        ScalarMulLayer(\n",
    "            torch.tensor([\n",
    "                [1.0, 1.0],\n",
    "                [1.0, -1.0]\n",
    "            ])\n",
    "        ),\n",
    "        AddLayer(torch.tensor([0.5, -0.5])),\n",
    "\n",
    ")\n",
    "\n",
    "model(torch.tensor([0.1, 0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the box abstraction to verify this network. Here are the rules:\n",
    "\n",
    "- Addition: $[a, b] +^\\# [c, d] = [a + c, b + d]$\n",
    "- Negation: $-^\\#[a, b] = [-b, -a]$\n",
    "- ReLU: $ReLU^\\#[a, b] = ReLU(a, b)$\n",
    "- Scalar multiplication: $\\lambda\\cdot^\\#[a, b] = [\\lambda\\cdot a, \\lambda\\cdot b]$ for $\\lambda > 0$\n",
    "\n",
    "A common pattern is to iterate over the layers in a module and build an \"abstract copy\" in parallel. One way of achieving this in PyTorch is to implement `nn.Modules` which perform the abstract operations.\n",
    "Note that the input for the new, abstract layers is slightly different from the old, concrete ones: We have to pass both the lower and upper bound. So, when we were passing a tensor of shape `(3,3)` before, we will\n",
    "now be passing one of shape `(2, 3, 3)` (we have a `3x3` tensor each, one for the lower and one upper bounds).\n",
    "Below we provide you with an example for the `AddLayer` and ask you to implement the missing layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractAddLayer(nn.Module):\n",
    "    def __init__(self, concrete_layer: AddLayer):\n",
    "        super(AbstractAddLayer, self).__init__()\n",
    "        self.bias = concrete_layer.bias\n",
    "\n",
    "    def forward(self, bounds):\n",
    "        return torch.sum(bounds, dim=-1) + self.bias\n",
    "    \n",
    "class AbstractScalarMulLayer(nn.Module):\n",
    "    def __init__(self, concrete_layer: ScalarMulLayer):\n",
    "        super(AbstractScalarMulLayer, self).__init__()\n",
    "        self.weights = concrete_layer.weights\n",
    "\n",
    "    def forward(self, bounds):\n",
    "        # TODO: Implement the forward pass of the box abstraction of the scalar multiplication layer\n",
    "        # NOTE: Make sure to handle negative weights correctly, i.e. using the negation rule!\n",
    "        lower = self.weights * bounds[0]\n",
    "        upper = self.weights * bounds[1]\n",
    "\n",
    "        # Where the weights are negative, we need to swap the lower and upper bound entries\n",
    "        return torch.stack([\n",
    "                torch.where(self.weights < 0, upper, lower), \n",
    "                torch.where(self.weights >= 0, upper, lower)\n",
    "            ])\n",
    "\n",
    "class AbstractReLULayer(nn.Module):\n",
    "    def __init__(self, _concrete_layer: ReLULayer):\n",
    "        super(AbstractReLULayer, self).__init__()\n",
    "\n",
    "    def forward(self, bounds):\n",
    "        # TODO: Implement the forward pass of the box abstraction of the ReLU layer\n",
    "        return torch.max(bounds, torch.zeros_like(bounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the layers implemented, you can now simply iterate over the original network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_model = []\n",
    "for layer in model:\n",
    "    if isinstance(layer, AddLayer):\n",
    "        abstract_model.append(AbstractAddLayer(layer))\n",
    "    elif isinstance(layer, ScalarMulLayer):\n",
    "        abstract_model.append(AbstractScalarMulLayer(layer))\n",
    "    elif isinstance(layer, ReLULayer):\n",
    "        abstract_model.append(AbstractReLULayer(layer))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown layer type: {layer}\")\n",
    "abstract_model = nn.Sequential(*abstract_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_5 in [0.6000000238418579, 1.4000000953674316] and o_6 in [-0.6000000238418579, 0.20000004768371582]\n"
     ]
    }
   ],
   "source": [
    "bounds = abstract_model(torch.tensor([[0.0, 0.1], [0.3, 0.4]]))\n",
    "print(f'o_5 in [{bounds[0, 0]}, {bounds[1, 0]}] and o_6 in [{bounds[0, 1]}, {bounds[1, 1]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we succeeded in proving that $o_5 > o_6$ for our given input ranges.\n",
    "What happens if you increase the input boxes to $[0, 0.6]$ and $[0.1, 0.7]$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_5 in [0.6000000238418579, 2.299999952316284] and o_6 in [-0.8999999761581421, 0.7999999523162842]\n"
     ]
    }
   ],
   "source": [
    "bounds = abstract_model(torch.tensor([[0, 0.1], [0.6, 0.7]]))\n",
    "print(f'o_5 in [{bounds[0, 0]}, {bounds[1, 0]}] and o_6 in [{bounds[0, 1]}, {bounds[1, 1]}]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, this fails :(\n",
    "\n",
    "This is why methods that are based on bounding box propagation are generally _incomplete_! Using Box, we fail to prove that the network classifies the inputs correctly, even though the property actually holds!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-lab-04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
