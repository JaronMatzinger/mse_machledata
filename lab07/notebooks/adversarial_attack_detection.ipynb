{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial attack detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cc7bdf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0, Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Torch version: {torch.__version__}, Device: {device.type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model to defend\n",
    "\n",
    "The reason for using MNIST is that we can train a good model on it in no time. The model below will reach 99% accuracy on the test set in just 5 epochs. More than enough to act as a little target to demonstrate adversarial detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "batch_size = 128\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),  # Converts PIL image or ndarray to a tensor image\n",
    "    v2.ToDtype(torch.float32, scale=True),  # Normalizes pixel values to [0, 1]\n",
    "    v2.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "clf = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "    print('Accuracy on the test set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.687\n",
      "[1,   200] loss: 0.153\n",
      "[1,   300] loss: 0.105\n",
      "[1,   400] loss: 0.085\n",
      "[2,   100] loss: 0.064\n",
      "[2,   200] loss: 0.061\n",
      "[2,   300] loss: 0.053\n",
      "[2,   400] loss: 0.048\n",
      "[3,   100] loss: 0.042\n",
      "[3,   200] loss: 0.042\n",
      "[3,   300] loss: 0.038\n",
      "[3,   400] loss: 0.035\n",
      "[4,   100] loss: 0.028\n",
      "[4,   200] loss: 0.029\n",
      "[4,   300] loss: 0.028\n",
      "[4,   400] loss: 0.029\n",
      "[5,   100] loss: 0.022\n",
      "[5,   200] loss: 0.019\n",
      "[5,   300] loss: 0.021\n",
      "[5,   400] loss: 0.025\n",
      "Accuracy on the test set: 99 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model and test it\n",
    "train(clf, train_loader, criterion, optimizer, epochs=5)\n",
    "test(clf, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in clf.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "clf.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defending the model\n",
    "\n",
    "Now that we have our target model, we can start implementing the idea we outlined in the introduction.\n",
    "Below you will find the VAE code from before. All that is left is updating the loss function.\n",
    "Remember, we want to solve the following optimization problem:\n",
    "\n",
    "$$\\min\\limits_\\theta D_{KL}(M(x) \\| M(AE_\\theta(x)))$$\n",
    "\n",
    "$M(x)$ is the output of the classifier on the input, and $M(AE(x))$ is the output of the classifier on the reconstructed input.\n",
    "For the KL divergence, use [`torch.nn.functional.kl_div`](https://pytorch.org/docs/stable/generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div). For numerical stability, set `kl_div`'s argument `log_target` to `True` and use `F.log_softmax` for the classifier outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) class.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input data.\n",
    "        hidden_dim (int): Dimensionality of the hidden layer.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    @dataclass\n",
    "    class VAEOutput:\n",
    "        \"\"\"\n",
    "        Dataclass for VAE output.\n",
    "        \n",
    "        Attributes:\n",
    "            z_dist (torch.distributions.Distribution): The distribution of the latent variable z.\n",
    "            z_sample (torch.Tensor): The sampled value of the latent variable z.\n",
    "            x_recon (torch.Tensor): The reconstructed output from the VAE.\n",
    "            loss (torch.Tensor): The overall loss of the VAE.\n",
    "            loss_recon (torch.Tensor): The reconstruction loss component of the VAE loss.\n",
    "            loss_kl (torch.Tensor): The KL divergence component of the VAE loss.\n",
    "        \"\"\"\n",
    "        z_dist: torch.distributions.Distribution\n",
    "        z_sample: torch.Tensor\n",
    "        x_recon: torch.Tensor\n",
    "        loss: torch.Tensor\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "                \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 8),\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(hidden_dim // 8, 2 * latent_dim), # 2 for mean and variance.\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 8),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 8, hidden_dim // 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def encode(self, x, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Encodes the input data into the latent space.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            eps (float): Small value to avoid numerical instability.\n",
    "        \n",
    "        Returns:\n",
    "            torch.distributions.MultivariateNormal: Normal distribution of the encoded data.\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=-1)\n",
    "        scale = self.softplus(logvar) + eps\n",
    "        scale_tril = torch.diag_embed(scale)\n",
    "        return torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)\n",
    "        \n",
    "    def reparameterize(self, dist):\n",
    "        \"\"\"\n",
    "        Reparameterizes the encoded data to sample from the latent space.\n",
    "        \n",
    "        Args:\n",
    "            dist (torch.distributions.MultivariateNormal): Normal distribution of the encoded data.\n",
    "        Returns:\n",
    "            torch.Tensor: Sampled data from the latent space.\n",
    "        \"\"\"\n",
    "        return dist.rsample()\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the data from the latent space to the original input space.\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Data in the latent space.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed data in the original input space.\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            compute_loss (bool): Whether to compute the loss or not.\n",
    "        \n",
    "        Returns:\n",
    "            VAEOutput: VAE output dataclass.\n",
    "        \"\"\"\n",
    "        dist = self.encode(x)\n",
    "        z = self.reparameterize(dist)\n",
    "        recon_x = self.decode(z)\n",
    "        \n",
    "        if not compute_loss:\n",
    "            return VAE.VAEOutput(\n",
    "                z_dist=dist,\n",
    "                z_sample=z,\n",
    "                x_recon=recon_x,\n",
    "                loss=None,\n",
    "            )\n",
    "\n",
    "        # TODO: Compute output probabilities for x, i.e. M(x)\n",
    "        probs = F.log_softmax(clf(x.view(-1, 1, 28, 28)), dim=1)\n",
    "\n",
    "        # TODO: Compute output probabilities for reconstructed x, i.e. M(AE(x))\n",
    "        probs_recon = F.log_softmax(clf(recon_x.view(-1, 1, 28, 28)), dim=1)\n",
    "        \n",
    "        # TODO: Compute loss term here.\n",
    "        loss = F.kl_div(probs, probs_recon, reduction='batchmean', log_target=True)\n",
    "\n",
    "        return VAE.VAEOutput(\n",
    "            z_dist=dist,\n",
    "            z_sample=z,\n",
    "            x_recon=recon_x,\n",
    "            loss=loss,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_data = datasets.MNIST(\n",
    "    './data', \n",
    "    download=False, \n",
    "    train=True, \n",
    "    transform=transform,\n",
    ")\n",
    "# Download and load the test data\n",
    "test_data = datasets.MNIST(\n",
    "    './data', \n",
    "    download=False, \n",
    "    train=False, \n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "vae = VAE(input_dim=784, hidden_dim=512, latent_dim=16).to(device)\n",
    "optimizer = torch.optim.AdamW(vae.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below you already know: we define the train and test functions, and fit the model to the training data. You can safely skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, prev_updates):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        data = data.view(data.size(0), -1)  # Flatten the data\n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        output = model(data)  # Forward pass\n",
    "        loss = output.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if n_upd % 100 == 0:\n",
    "            # Calculate and log gradient norms\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "            print(f'Step {n_upd:,} (N samples: {n_upd*batch_size:,}), Loss: {loss.item():.4f}, Grad: {total_norm:.4f}')\n",
    "            \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    \"\"\"\n",
    "    Tests the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        cur_step (int): The current step.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing'):\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)  # Flatten the data\n",
    "            \n",
    "            output = model(data, compute_loss=True)  # Forward pass\n",
    "            \n",
    "            test_loss += output.loss.item()\n",
    "            \n",
    "    test_loss /= len(dataloader)\n",
    "    print(f'====> Test set loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]/var/folders/b6/tgpndp7n76s9pccqsl6nzlfr0000gn/T/ipykernel_9712/2474640629.py:128: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.log_softmax(clf(x.view(-1, 1, 28, 28)))\n",
      "/var/folders/b6/tgpndp7n76s9pccqsl6nzlfr0000gn/T/ipykernel_9712/2474640629.py:131: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs_recon = F.log_softmax(clf(recon_x.view(-1, 1, 28, 28)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (N samples: 0), Loss: 13.9894, Grad: 0.1516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 112/938 [00:02<00:13, 62.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 (N samples: 6,400), Loss: 9.9518, Grad: 6.5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 210/938 [00:04<00:11, 62.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 (N samples: 12,800), Loss: 12.8497, Grad: 2.4934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 308/938 [00:05<00:10, 59.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300 (N samples: 19,200), Loss: 7.2079, Grad: 7.9161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 413/938 [00:07<00:08, 62.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400 (N samples: 25,600), Loss: 5.9322, Grad: 1.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 511/938 [00:09<00:06, 61.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 (N samples: 32,000), Loss: 7.6862, Grad: 0.8046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 609/938 [00:10<00:05, 61.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600 (N samples: 38,400), Loss: 5.6189, Grad: 11.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 707/938 [00:12<00:03, 60.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700 (N samples: 44,800), Loss: 7.4210, Grad: 7.2534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 811/938 [00:13<00:02, 59.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800 (N samples: 51,200), Loss: 6.5895, Grad: 4.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 907/938 [00:15<00:00, 57.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900 (N samples: 57,600), Loss: 5.1196, Grad: 7.1781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:16<00:00, 56.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 72/938 [00:01<00:14, 59.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,000 (N samples: 64,000), Loss: 4.6401, Grad: 1.3983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 170/938 [00:02<00:12, 60.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,100 (N samples: 70,400), Loss: 4.9910, Grad: 8.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 268/938 [00:04<00:11, 56.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,200 (N samples: 76,800), Loss: 7.1417, Grad: 25.9015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 372/938 [00:06<00:10, 53.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,300 (N samples: 83,200), Loss: 5.5544, Grad: 2.8940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 471/938 [00:08<00:08, 56.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,400 (N samples: 89,600), Loss: 5.7195, Grad: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 571/938 [00:09<00:05, 62.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,500 (N samples: 96,000), Loss: 5.1553, Grad: 0.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 669/938 [00:11<00:04, 62.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,600 (N samples: 102,400), Loss: 4.9911, Grad: 3.3481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 772/938 [00:12<00:02, 58.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,700 (N samples: 108,800), Loss: 4.3261, Grad: 14.2767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 873/938 [00:14<00:01, 59.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,800 (N samples: 115,200), Loss: 4.5705, Grad: 1.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 59.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 35/938 [00:00<00:15, 56.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1,900 (N samples: 121,600), Loss: 3.7350, Grad: 2.4948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 132/938 [00:02<00:13, 61.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,000 (N samples: 128,000), Loss: 4.0344, Grad: 4.2623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 237/938 [00:03<00:10, 63.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,100 (N samples: 134,400), Loss: 5.7955, Grad: 7.7073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 335/938 [00:05<00:09, 64.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,200 (N samples: 140,800), Loss: 4.2672, Grad: 6.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 433/938 [00:06<00:08, 61.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,300 (N samples: 147,200), Loss: 4.7916, Grad: 3.5212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 531/938 [00:08<00:06, 61.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,400 (N samples: 153,600), Loss: 2.6690, Grad: 1.2665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 635/938 [00:10<00:05, 60.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,500 (N samples: 160,000), Loss: 4.3630, Grad: 2.4722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 733/938 [00:11<00:03, 63.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,600 (N samples: 166,400), Loss: 4.4012, Grad: 81.9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 831/938 [00:13<00:01, 59.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,700 (N samples: 172,800), Loss: 5.1275, Grad: 1.5782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 62.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,800 (N samples: 179,200), Loss: 5.8947, Grad: 0.8308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 98/938 [00:01<00:13, 60.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2,900 (N samples: 185,600), Loss: 4.9353, Grad: 0.7834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 196/938 [00:03<00:12, 60.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,000 (N samples: 192,000), Loss: 4.1315, Grad: 3.3277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 293/938 [00:04<00:10, 59.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,100 (N samples: 198,400), Loss: 4.3860, Grad: 15.4260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 397/938 [00:06<00:08, 62.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,200 (N samples: 204,800), Loss: 4.3867, Grad: 9.9663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 495/938 [00:08<00:07, 58.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,300 (N samples: 211,200), Loss: 4.7444, Grad: 6.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 598/938 [00:09<00:05, 57.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,400 (N samples: 217,600), Loss: 5.8735, Grad: 0.2316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 694/938 [00:11<00:03, 62.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,500 (N samples: 224,000), Loss: 6.2540, Grad: 2.0540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 799/938 [00:12<00:02, 63.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,600 (N samples: 230,400), Loss: 5.3923, Grad: 0.4164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 890/938 [00:14<00:00, 52.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,700 (N samples: 236,800), Loss: 3.7721, Grad: 1.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 61.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 61/938 [00:01<00:14, 61.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,800 (N samples: 243,200), Loss: 3.5500, Grad: 8.9268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 155/938 [00:02<00:13, 59.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3,900 (N samples: 249,600), Loss: 4.6246, Grad: 0.6913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 260/938 [00:04<00:11, 59.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,000 (N samples: 256,000), Loss: 5.2181, Grad: 0.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 355/938 [00:05<00:09, 60.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,100 (N samples: 262,400), Loss: 3.3302, Grad: 0.3269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 460/938 [00:07<00:07, 59.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,200 (N samples: 268,800), Loss: 3.4209, Grad: 3.2068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 557/938 [00:09<00:06, 63.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,300 (N samples: 275,200), Loss: 2.8852, Grad: 0.2665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 655/938 [00:10<00:04, 62.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,400 (N samples: 281,600), Loss: 3.8163, Grad: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 760/938 [00:12<00:02, 60.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,500 (N samples: 288,000), Loss: 4.4376, Grad: 6.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 857/938 [00:13<00:01, 59.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,600 (N samples: 294,400), Loss: 2.8381, Grad: 0.8518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 61.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/938 [00:00<00:15, 57.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,700 (N samples: 300,800), Loss: 4.7105, Grad: 0.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 118/938 [00:01<00:12, 63.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,800 (N samples: 307,200), Loss: 3.5840, Grad: 0.8259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 223/938 [00:03<00:11, 62.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4,900 (N samples: 313,600), Loss: 3.4100, Grad: 3.3119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 321/938 [00:05<00:09, 63.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,000 (N samples: 320,000), Loss: 4.5350, Grad: 3.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 419/938 [00:06<00:08, 63.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,100 (N samples: 326,400), Loss: 4.5928, Grad: 0.4453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 517/938 [00:08<00:06, 62.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,200 (N samples: 332,800), Loss: 4.6300, Grad: 66.3389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 622/938 [00:09<00:04, 63.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,300 (N samples: 339,200), Loss: 5.2838, Grad: 0.4460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 720/938 [00:11<00:03, 63.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,400 (N samples: 345,600), Loss: 5.3280, Grad: 1.6859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 818/938 [00:13<00:01, 62.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,500 (N samples: 352,000), Loss: 4.0889, Grad: 10.5286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 923/938 [00:14<00:00, 63.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,600 (N samples: 358,400), Loss: 3.2968, Grad: 0.4917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 62.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 84/938 [00:01<00:13, 63.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,700 (N samples: 364,800), Loss: 3.6241, Grad: 0.6037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 182/938 [00:02<00:12, 62.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,800 (N samples: 371,200), Loss: 4.4905, Grad: 0.4001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 280/938 [00:04<00:10, 62.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5,900 (N samples: 377,600), Loss: 4.6319, Grad: 2.0915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 385/938 [00:06<00:08, 62.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,000 (N samples: 384,000), Loss: 4.2956, Grad: 3.1392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 483/938 [00:07<00:07, 63.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,100 (N samples: 390,400), Loss: 4.5527, Grad: 34.5696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 581/938 [00:09<00:05, 62.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,200 (N samples: 396,800), Loss: 4.1521, Grad: 1.8973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 679/938 [00:10<00:04, 61.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,300 (N samples: 403,200), Loss: 4.7802, Grad: 4.9007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 784/938 [00:12<00:02, 61.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,400 (N samples: 409,600), Loss: 2.3331, Grad: 1.7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 882/938 [00:14<00:00, 62.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,500 (N samples: 416,000), Loss: 3.7337, Grad: 0.8958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 62.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/938 [00:00<00:14, 61.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,600 (N samples: 422,400), Loss: 2.8906, Grad: 19.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 146/938 [00:02<00:12, 62.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,700 (N samples: 428,800), Loss: 2.8031, Grad: 0.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 244/938 [00:03<00:11, 62.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,800 (N samples: 435,200), Loss: 3.1556, Grad: 0.4888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 342/938 [00:05<00:09, 60.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6,900 (N samples: 441,600), Loss: 4.4667, Grad: 1.7947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 443/938 [00:07<00:07, 62.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,000 (N samples: 448,000), Loss: 3.1242, Grad: 1.4888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 548/938 [00:08<00:06, 64.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,100 (N samples: 454,400), Loss: 3.2385, Grad: 0.5351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 646/938 [00:10<00:04, 63.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,200 (N samples: 460,800), Loss: 2.4469, Grad: 0.7687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 744/938 [00:11<00:03, 63.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,300 (N samples: 467,200), Loss: 1.7489, Grad: 0.5218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 842/938 [00:13<00:01, 64.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,400 (N samples: 473,600), Loss: 3.2219, Grad: 2.3013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 63.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,500 (N samples: 480,000), Loss: 2.6736, Grad: 0.4206\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 105/938 [00:01<00:13, 63.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,600 (N samples: 486,400), Loss: 2.4367, Grad: 2.8232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 203/938 [00:03<00:11, 64.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,700 (N samples: 492,800), Loss: 3.6664, Grad: 0.2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 308/938 [00:04<00:09, 63.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,800 (N samples: 499,200), Loss: 3.1392, Grad: 8.3068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 406/938 [00:06<00:08, 62.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7,900 (N samples: 505,600), Loss: 2.3260, Grad: 0.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 504/938 [00:07<00:07, 60.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,000 (N samples: 512,000), Loss: 4.3814, Grad: 27.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 602/938 [00:09<00:05, 63.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,100 (N samples: 518,400), Loss: 3.2630, Grad: 0.3645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 706/938 [00:11<00:03, 61.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,200 (N samples: 524,800), Loss: 3.9503, Grad: 1.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 804/938 [00:12<00:02, 61.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,300 (N samples: 531,200), Loss: 2.6088, Grad: 0.1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 909/938 [00:14<00:00, 64.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,400 (N samples: 537,600), Loss: 2.9118, Grad: 0.6662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 63.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 70/938 [00:01<00:14, 58.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,500 (N samples: 544,000), Loss: 3.9944, Grad: 3.6938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 167/938 [00:02<00:13, 58.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,600 (N samples: 550,400), Loss: 3.3017, Grad: 1.0981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 266/938 [00:04<00:11, 58.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,700 (N samples: 556,800), Loss: 2.6926, Grad: 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 371/938 [00:06<00:09, 62.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,800 (N samples: 563,200), Loss: 3.0443, Grad: 0.3062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 469/938 [00:07<00:07, 63.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8,900 (N samples: 569,600), Loss: 3.7474, Grad: 8.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 567/938 [00:09<00:06, 61.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,000 (N samples: 576,000), Loss: 2.1953, Grad: 0.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 671/938 [00:11<00:04, 57.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,100 (N samples: 582,400), Loss: 3.6002, Grad: 2.3359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 767/938 [00:12<00:02, 60.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,200 (N samples: 588,800), Loss: 2.6540, Grad: 24.4786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 865/938 [00:14<00:01, 58.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,300 (N samples: 595,200), Loss: 3.0182, Grad: 0.4210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 60.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/938 [00:00<00:17, 50.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,400 (N samples: 601,600), Loss: 3.1483, Grad: 0.2776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 130/938 [00:02<00:13, 60.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,500 (N samples: 608,000), Loss: 2.7225, Grad: 5.0967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 228/938 [00:03<00:11, 63.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,600 (N samples: 614,400), Loss: 3.0617, Grad: 21.2308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 331/938 [00:05<00:09, 61.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,700 (N samples: 620,800), Loss: 1.4669, Grad: 1.2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 429/938 [00:07<00:08, 60.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,800 (N samples: 627,200), Loss: 3.6008, Grad: 1.3120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 527/938 [00:08<00:06, 60.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9,900 (N samples: 633,600), Loss: 3.7506, Grad: 1.9387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 631/938 [00:10<00:05, 59.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,000 (N samples: 640,000), Loss: 2.2190, Grad: 0.3150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 729/938 [00:11<00:03, 62.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,100 (N samples: 646,400), Loss: 3.5445, Grad: 97.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 827/938 [00:13<00:01, 58.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,200 (N samples: 652,800), Loss: 2.9959, Grad: 0.4624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 932/938 [00:15<00:00, 60.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,300 (N samples: 659,200), Loss: 4.3887, Grad: 0.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 61.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 91/938 [00:01<00:13, 63.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,400 (N samples: 665,600), Loss: 3.8654, Grad: 0.4257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 189/938 [00:02<00:11, 63.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,500 (N samples: 672,000), Loss: 3.1204, Grad: 35.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 294/938 [00:04<00:10, 62.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,600 (N samples: 678,400), Loss: 2.4848, Grad: 0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 392/938 [00:06<00:08, 61.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,700 (N samples: 684,800), Loss: 3.3419, Grad: 1.3992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 489/938 [00:07<00:07, 58.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,800 (N samples: 691,200), Loss: 2.4560, Grad: 0.7456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 594/938 [00:09<00:05, 63.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10,900 (N samples: 697,600), Loss: 2.4492, Grad: 4.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 690/938 [00:10<00:04, 57.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,000 (N samples: 704,000), Loss: 2.3339, Grad: 0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 793/938 [00:12<00:02, 60.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,100 (N samples: 710,400), Loss: 3.2655, Grad: 1.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 890/938 [00:14<00:00, 60.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,200 (N samples: 716,800), Loss: 3.1895, Grad: 1.9561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 62.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 55/938 [00:00<00:14, 58.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,300 (N samples: 723,200), Loss: 2.8872, Grad: 0.2862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 151/938 [00:02<00:13, 60.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,400 (N samples: 729,600), Loss: 3.4096, Grad: 7.7934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 256/938 [00:04<00:11, 60.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,500 (N samples: 736,000), Loss: 1.9562, Grad: 1.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 354/938 [00:05<00:09, 63.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,600 (N samples: 742,400), Loss: 2.3484, Grad: 0.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 452/938 [00:07<00:08, 56.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,700 (N samples: 748,800), Loss: 2.3615, Grad: 0.1774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 557/938 [00:09<00:06, 60.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,800 (N samples: 755,200), Loss: 2.2285, Grad: 0.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 655/938 [00:10<00:04, 60.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11,900 (N samples: 761,600), Loss: 3.6729, Grad: 0.4528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 752/938 [00:12<00:02, 62.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,000 (N samples: 768,000), Loss: 2.0655, Grad: 0.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 857/938 [00:13<00:01, 63.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,100 (N samples: 774,400), Loss: 2.4704, Grad: 2.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 61.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/938 [00:00<00:15, 58.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,200 (N samples: 780,800), Loss: 3.3333, Grad: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 112/938 [00:01<00:13, 61.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,300 (N samples: 787,200), Loss: 2.7410, Grad: 9.5727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 217/938 [00:03<00:11, 60.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,400 (N samples: 793,600), Loss: 2.5773, Grad: 1.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 315/938 [00:05<00:11, 54.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,500 (N samples: 800,000), Loss: 3.2918, Grad: 0.1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 418/938 [00:06<00:08, 62.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,600 (N samples: 806,400), Loss: 3.3539, Grad: 0.3422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 519/938 [00:08<00:07, 59.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,700 (N samples: 812,800), Loss: 3.2837, Grad: 0.7449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 614/938 [00:10<00:05, 59.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,800 (N samples: 819,200), Loss: 2.9210, Grad: 0.2575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 719/938 [00:11<00:03, 61.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12,900 (N samples: 825,600), Loss: 2.8849, Grad: 0.1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 817/938 [00:13<00:01, 60.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,000 (N samples: 832,000), Loss: 2.4958, Grad: 1.4110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 915/938 [00:14<00:00, 63.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,100 (N samples: 838,400), Loss: 2.2572, Grad: 2.3176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 61.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 77/938 [00:01<00:14, 61.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,200 (N samples: 844,800), Loss: 3.8639, Grad: 1.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 175/938 [00:02<00:12, 61.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,300 (N samples: 851,200), Loss: 2.3085, Grad: 0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 278/938 [00:04<00:10, 60.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,400 (N samples: 857,600), Loss: 2.8849, Grad: 23.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 376/938 [00:06<00:08, 63.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,500 (N samples: 864,000), Loss: 2.4419, Grad: 0.5053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 481/938 [00:07<00:07, 64.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,600 (N samples: 870,400), Loss: 4.4251, Grad: 24.5119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 578/938 [00:09<00:06, 57.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,700 (N samples: 876,800), Loss: 3.9162, Grad: 9.5778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 675/938 [00:10<00:04, 60.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,800 (N samples: 883,200), Loss: 3.6677, Grad: 20.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 780/938 [00:12<00:02, 59.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13,900 (N samples: 889,600), Loss: 4.4148, Grad: 4.1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 876/938 [00:14<00:01, 60.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,000 (N samples: 896,000), Loss: 2.6850, Grad: 0.0898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 61.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 42/938 [00:00<00:14, 61.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,100 (N samples: 902,400), Loss: 2.7749, Grad: 0.5948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 139/938 [00:02<00:13, 58.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,200 (N samples: 908,800), Loss: 3.0707, Grad: 0.5846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 244/938 [00:03<00:10, 63.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,300 (N samples: 915,200), Loss: 2.5445, Grad: 1.4460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 338/938 [00:05<00:09, 60.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,400 (N samples: 921,600), Loss: 3.1997, Grad: 3.1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 441/938 [00:07<00:08, 58.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,500 (N samples: 928,000), Loss: 3.9784, Grad: 1.1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 539/938 [00:08<00:06, 61.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,600 (N samples: 934,400), Loss: 2.1882, Grad: 0.2957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 637/938 [00:10<00:04, 61.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,700 (N samples: 940,800), Loss: 3.6189, Grad: 19.5231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 741/938 [00:12<00:03, 60.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,800 (N samples: 947,200), Loss: 3.5010, Grad: 0.5937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 839/938 [00:13<00:01, 62.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14,900 (N samples: 953,600), Loss: 3.6705, Grad: 2.5502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 61.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,000 (N samples: 960,000), Loss: 3.6347, Grad: 0.1911\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 105/938 [00:01<00:12, 64.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,100 (N samples: 966,400), Loss: 2.6127, Grad: 12.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 203/938 [00:03<00:11, 63.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,200 (N samples: 972,800), Loss: 3.2516, Grad: 11.5361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 301/938 [00:04<00:09, 64.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,300 (N samples: 979,200), Loss: 2.8004, Grad: 0.6304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 399/938 [00:06<00:08, 63.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,400 (N samples: 985,600), Loss: 4.9900, Grad: 15.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 504/938 [00:07<00:07, 56.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,500 (N samples: 992,000), Loss: 3.3419, Grad: 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 600/938 [00:09<00:05, 62.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,600 (N samples: 998,400), Loss: 3.3496, Grad: 8.1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 704/938 [00:11<00:03, 61.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,700 (N samples: 1,004,800), Loss: 2.9707, Grad: 1.3768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 801/938 [00:12<00:02, 54.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,800 (N samples: 1,011,200), Loss: 3.4606, Grad: 0.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 905/938 [00:14<00:00, 61.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15,900 (N samples: 1,017,600), Loss: 2.8317, Grad: 0.1283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 62.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 63/938 [00:01<00:13, 63.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,000 (N samples: 1,024,000), Loss: 4.1043, Grad: 9.2902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 161/938 [00:02<00:14, 52.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,100 (N samples: 1,030,400), Loss: 4.5844, Grad: 6.8695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 265/938 [00:04<00:11, 61.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,200 (N samples: 1,036,800), Loss: 4.2821, Grad: 107.0639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 363/938 [00:06<00:09, 59.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,300 (N samples: 1,043,200), Loss: 2.2899, Grad: 0.6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 465/938 [00:07<00:07, 61.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,400 (N samples: 1,049,600), Loss: 3.2669, Grad: 1.4988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 562/938 [00:09<00:06, 58.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,500 (N samples: 1,056,000), Loss: 3.0604, Grad: 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 666/938 [00:11<00:04, 59.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,600 (N samples: 1,062,400), Loss: 3.9747, Grad: 0.7877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 763/938 [00:12<00:02, 61.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,700 (N samples: 1,068,800), Loss: 4.0741, Grad: 0.0385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 861/938 [00:14<00:01, 63.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,800 (N samples: 1,075,200), Loss: 3.3931, Grad: 2.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 60.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/938 [00:00<00:14, 61.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16,900 (N samples: 1,081,600), Loss: 4.8900, Grad: 0.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 125/938 [00:02<00:13, 61.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,000 (N samples: 1,088,000), Loss: 2.5095, Grad: 11.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 228/938 [00:03<00:12, 59.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,100 (N samples: 1,094,400), Loss: 3.3955, Grad: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 325/938 [00:05<00:09, 64.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,200 (N samples: 1,100,800), Loss: 3.1330, Grad: 1.6958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 423/938 [00:06<00:08, 60.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,300 (N samples: 1,107,200), Loss: 1.8756, Grad: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 528/938 [00:08<00:06, 61.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,400 (N samples: 1,113,600), Loss: 2.7264, Grad: 11.1842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 626/938 [00:10<00:04, 64.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,500 (N samples: 1,120,000), Loss: 4.1639, Grad: 17.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 724/938 [00:11<00:03, 62.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,600 (N samples: 1,126,400), Loss: 2.9140, Grad: 16.8963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 829/938 [00:13<00:01, 62.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,700 (N samples: 1,132,800), Loss: 2.9580, Grad: 0.9509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 927/938 [00:14<00:00, 62.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,800 (N samples: 1,139,200), Loss: 3.7197, Grad: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 62.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 91/938 [00:01<00:13, 64.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17,900 (N samples: 1,145,600), Loss: 4.3778, Grad: 8.2795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 189/938 [00:02<00:12, 62.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18,000 (N samples: 1,152,000), Loss: 4.3515, Grad: 41.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 287/938 [00:04<00:10, 61.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18,100 (N samples: 1,158,400), Loss: 4.4296, Grad: 51.9968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 385/938 [00:06<00:09, 60.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18,200 (N samples: 1,164,800), Loss: 3.1301, Grad: 26.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 490/938 [00:07<00:07, 63.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18,300 (N samples: 1,171,200), Loss: 3.7025, Grad: 6.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 586/938 [00:09<00:05, 59.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18,400 (N samples: 1,177,600), Loss: 3.0807, Grad: 1.1452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 690/938 [00:11<00:03, 62.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18,500 (N samples: 1,184,000), Loss: 2.1908, Grad: 19.7727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 788/938 [00:12<00:02, 60.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18,600 (N samples: 1,190,400), Loss: 3.4305, Grad: 0.0801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 891/938 [00:14<00:00, 59.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18,700 (N samples: 1,196,800), Loss: 2.4168, Grad: 0.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:15<00:00, 62.15it/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 83.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 2.9867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): SiLU()\n",
       "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (7): SiLU()\n",
       "    (8): Linear(in_features=512, out_features=784, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "prev_updates = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    prev_updates = train(vae, train_loader, optimizer, prev_updates)\n",
    "test(vae, test_loader)\n",
    "\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'adversarial_vae.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Testing our defense\n",
    "\n",
    "To test our defense, we first have to generate adversarial examples. In a previous lab, we used the Fast Gradient Sign Method (FGSM) to generate adversarial examples. This time, we will use a few more methods, and we will use the `foolbox` library to do so. The library is very easy to use and has a lot of built-in methods to generate adversarial examples.\n",
    "\n",
    "Installing it is as easy as running `pip install foolbox`. You can find the documentation [here](https://foolbox.readthedocs.io/en/latest/). We've already installed it for you.\n",
    "\n",
    "The testing procedure is as follows:\n",
    "1. Generate adversarial examples using the `foolbox` library.\n",
    "2. Compute the accuracy of the classifier on the adversarial examples.\n",
    "3. Pass the adversarial examples through the VAE.\n",
    "4. Pass the reconstructed examples through the classifier and compute the accuracy.\n",
    "5. Compare the accuracy of the adversarial examples and the reconstructed adversarial examples.\n",
    "\n",
    "### 1. Generating adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: foolbox in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (3.3.4)\n",
      "Requirement already satisfied: numpy in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from foolbox) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from foolbox) (1.15.2)\n",
      "Requirement already satisfied: setuptools in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from foolbox) (78.1.0)\n",
      "Requirement already satisfied: eagerpy>=0.30.0 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from foolbox) (0.30.0)\n",
      "Requirement already satisfied: GitPython>=3.0.7 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from foolbox) (3.1.44)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from foolbox) (4.13.1)\n",
      "Requirement already satisfied: requests>=2.24.0 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from foolbox) (2.32.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from GitPython>=3.0.7->foolbox) (4.0.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from requests>=2.24.0->foolbox) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from requests>=2.24.0->foolbox) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from requests>=2.24.0->foolbox) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from requests>=2.24.0->foolbox) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/jaron/miniconda3/envs/mlops-lab-07/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython>=3.0.7->foolbox) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install foolbox\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "from foolbox.attacks import LinfPGD\n",
    "\n",
    "# Wrap the classifier with Foolbox\n",
    "fclf = PyTorchModel(clf, bounds=(-1, 1))\n",
    "\n",
    "# Get a batch of test data\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# Attack the classifier\n",
    "attack = LinfPGD()\n",
    "raw_advs, clipped_advs, success = attack(fclf, images, labels, epsilons=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute the accuracy of the classifier on the adversarial examples\n",
    "_Hint: You don't have to run the classifier again. Instead, you can use the `success` variable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the accuracy of the classifier on the adversarial examples\n",
    "accuracy_adv = 1 - success.to(torch.float32).mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pass the adversarial examples through the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pass the adversarial examples through the VAE\n",
    "# Hint: Use `clipped_advs`\n",
    "advs = clipped_advs.view(raw_advs.size(0), -1)\n",
    "advs = advs.to(device)\n",
    "advs_recon = vae(advs, compute_loss=False).x_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pass the reconstructed examples through the classifier and compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pass the reconstructed examples through the classifier and compute the accuracy\n",
    "# Hint: Use `advs_reconstructed`\n",
    "advs_recon = advs_recon.view(-1, 1, 28, 28)\n",
    "advs_recon = advs_recon.to(device)\n",
    "clf.to(device)\n",
    "outputs = clf(advs_recon)\n",
    "_, predicted = torch.max(outputs.data, dim=1)\n",
    "accuracy_recon = (predicted == labels.to(device)).sum().item() / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compare the accuracy of the adversarial examples and the reconstructed adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial examples: 0.5469\n",
      "Accuracy on reconstructed examples: 0.6719\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the accuracy of the classifier on the adversarial examples and the reconstructed examples\n",
    "print(f'Accuracy on adversarial examples: {accuracy_adv:.4f}')\n",
    "print(f'Accuracy on reconstructed examples: {accuracy_recon:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What conclusions can you draw from the results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-lab-07",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
