{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the ü§ó `transformers` library\n",
    "\n",
    "As mentioned in our glorious introduction, we are going to use the [Hugging Face Hub](https://huggingface.co/docs/hub/index) today.\n",
    "Let's start with some numbers. At the time of writing, the Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps.\n",
    "Everything is open source and publicly available.\n",
    "\n",
    "Under the hood, the Hugging Face Hub uses git-based repositories. While you can directly interact with Hub repos as you would with any other git repo, we will be using the Hugging Face `transformers` library.\n",
    "\n",
    "Installing the transformers library is as easy as\n",
    "\n",
    "```shell\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "You also need your favourite deep learning framework (`torch` or `tensorflow`). Today you don't have to worry about dependencies, we've already added everything to the `environment.yaml`.\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "With `transformers`, all (NLP) models are a line away from being used. You can choose between a high- and a low-level API, depending on how you want to use them.\n",
    "On the highest level, there is the `pipeline` function. `pipeline` can be used for most tasks across different modalities. Here are some examples, you can find the complete list in the [`transformers` documentation](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline).\n",
    "\n",
    "| Task                      | Description                                                     | Modality           | Pipeline identifier                           |\n",
    "|---------------------------|-----------------------------------------------------------------|--------------------|------------------------------------------------|\n",
    "| Text classification       | assign a label to a given sequence of text                      | NLP                | `pipeline(task=‚Äúsentiment-analysis‚Äù)`            |\n",
    "| Text generation           | generate text given a prompt                                     | NLP                | `pipeline(task=‚Äútext-generation‚Äù)`               |\n",
    "| Summarization             | generate a summary of a sequence of text or document             | NLP                | `pipeline(task=‚Äúsummarization‚Äù)`                 |\n",
    "| Image classification      | assign a label to an image                                       | Computer vision    | `pipeline(task=‚Äúimage-classification‚Äù)`          |\n",
    "| Image segmentation        | assign a label to each individual pixel of an image               | Computer vision    | `pipeline(task=‚Äúimage-segmentation‚Äù)`            |\n",
    "| Object detection          | predict the bounding boxes and classes of objects in an image     | Computer vision    | `pipeline(task=‚Äúobject-detection‚Äù)`              |\n",
    "| Audio classification      | assign a label to some audio data                                 | Audio              | `pipeline(task=‚Äúaudio-classification‚Äù)`          |\n",
    "| Automatic speech recognition | transcribe speech into text                                    | Audio              | `pipeline(task=‚Äúautomatic-speech-recognition‚Äù)` |\n",
    "| Visual question answering | answer a question about the image, given an image and a question | Multi-modal        | `pipeline(task=‚Äúvqa‚Äù)`                           |\n",
    "| Document question answering | answer a question about the document, given a document and a question | Multi-modal    | `pipeline(task=‚Äúdocument-question-answering‚Äù)`  |\n",
    "| Image captioning          | generate a caption for a given image                             | Multi-modal        | `pipeline(task=‚Äúimage-to-text‚Äù)`                 |\n",
    "\n",
    "Let's use `sentiment-analysis` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (4.48.1)\n",
      "Requirement already satisfied: filelock in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaron/miniconda3/envs/mlops-lab-02/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `pipeline` downloads and caches a pretrained model as well as a tokenizer. You can now use the `classifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998689889907837}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am thrilled to announce that coffee is the best caffeinated drink in the world.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at `classifier`. `classifier.model` will tell you which model the pipeline is using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from the previous lab that there are two important concepts: The tokenizer and the model itself. If you need a refresher:\n",
    "\n",
    "Tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table. You can learn more about tokenizers in the [Hugging Face docs](https://huggingface.co/docs/transformers/tokenizer_summary).\n",
    "\n",
    "You can also access a model's the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if you want to use a particular model? You can pass an optional `model` parameter to `pipeline`. Let's say you wanted to analyze French text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7428178787231445}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"J'ai le plaisir d'annoncer que le caf√© est la meilleure boisson caf√©in√©e au monde.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there's no guarantee that the output of different models is the same, and it's best to read the documentation page of a particular model. Speaking of documentation pages, models on the Hugging Face Hub come with what is known as [_Model Card_](https://huggingface.co/docs/hub/model-cards).\n",
    "\n",
    "> Model cards are files that accompany the models and provide handy information. Under the hood, model cards are simple Markdown files with additional metadata. Model cards are essential for discoverability, reproducibility, and sharing! You can find a model card as the README.md file in any model repo. ([docs](https://huggingface.co/docs/hub/model-cards))\n",
    "\n",
    "Had we read [the model card for ` bert-base-multilingual-uncased-sentiment `]https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) beforehand, we would not have been surprised by the return value. It was trained on reviews, no wonder it returns star ratings!\n",
    "\n",
    "## `AutoClass`es\n",
    "\n",
    "As we've learnt last week, for (large) language models to work, we need a _tokenizer_ and the _model_ itself. These two concepts are implemented via the `Tokenizer` and `Model` base classes respectively, which the concrete models and their tokenizers inherit from.\n",
    "\n",
    "Whenever you need a particular model and want more control over it than you get by invoking `pipeline(...)`, you can instantiate it via its class. For instance, for the `nlptown/bert-base-multilingual-uncased-sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#¬†The model...\n",
    "BertForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='nlptown/bert-base-multilingual-uncased-sentiment', vocab_size=105879, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#¬†... and its tokenizer\n",
    "BertTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be very tedious, however. Remember, there are some 350k models on Hugging Face Hub and its not always obvious which model class is the right one for your model. So, how do we overcome this issue?\n",
    "\n",
    "Enter `AutoClasses`: An `AutoClass` is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate `AutoClass` for your task and it‚Äôs associated preprocessing class.\n",
    "So, again, for `nlptown/bert-base-multilingual-uncased-sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "#¬†The model...\n",
    "AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='nlptown/bert-base-multilingual-uncased-sentiment', vocab_size=105879, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#¬†... and its tokenizer\n",
    "AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't this much more convenient? `AutoClasses` also power `pipeline`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    ),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\n",
    "        \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7832970023155212}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Sono lieto di annunciare che il caff√® √® la migliore bevanda a base di caffeina del mondo.\")\n",
    "#¬†... or so, my Italian is not that good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is everything you need to know about `transformers` for the moment. We will cover optimized inference and training large models later.\n",
    "\n",
    "Now, let's engineer some prompts!\n",
    "\n",
    "## Prompt Engineering 101\n",
    "\n",
    "What is prompt engineering? According to Wikipedia:\n",
    "\n",
    "> Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.\n",
    "\n",
    "In this section, we are going to engineer some prompts to make the LLM answer all our questions. We'll use the \"tiny\" LLM [`Qwen/Qwen2.5-0.5B`](https://huggingface.co/Qwen/Qwen2.5-0.5B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "qwen = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-0.5B\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is the task that qwen should perform for us? Answering questions, of course. Let's describe this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "\"\"\"\n",
    "You are a helpful bot and are answering all questions the human has. \n",
    "You only answer the question and do not provide any additional information. \n",
    "You are not allowed to ask questions.\n",
    "\n",
    "The human asks: \"{question}\"\n",
    "\n",
    "Your answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the `{question}` template? We can use python's string formatting to fill it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful bot and are answering all questions the human has. \n",
      "You only answer the question and do not provide any additional information. \n",
      "You are not allowed to ask questions.\n",
      "\n",
      "The human asks: \"What is the meaning of life?\"\n",
      "\n",
      "Your answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(question=\"What is the meaning of life?\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask qwen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful bot and are answering all questions the human has. \n",
      "You only answer the question and do not provide any additional information. \n",
      "You are not allowed to ask questions.\n",
      "\n",
      "The human asks: \"What is the meaning of life?\"\n",
      "\n",
      "Your answer: \"The meaning of life is to live a meaningful and fulfilling life, to find purpose and meaning in one's existence, and to strive for a higher level of consciousness and self-awareness.\"\n"
     ]
    }
   ],
   "source": [
    "print(qwen(prompt.format(question=\"What is the meaning of life?\").strip())[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn: Tune the prompt such that it only answers the question and doesn't write anything beyond it.\n",
    "\n",
    "### Leveraging Prompt Engineering to add context\n",
    "\n",
    "How can we make qwen answer questions about facts? Again, we engineer the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "\"\"\"\n",
    "You are a helpful bot and are answering all questions the human has. \n",
    "You only answer the question and do not provide any additional information. \n",
    "You are not allowed to ask questions.\n",
    "\n",
    "You are given the following context:\n",
    "Big Thought's favorite color is blue.\n",
    "\n",
    "\n",
    "The human asks: \"{question}\"\n",
    "\n",
    "Your answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful bot and are answering all questions the human has. \n",
      "You only answer the question and do not provide any additional information. \n",
      "You are not allowed to ask questions.\n",
      "\n",
      "You are given the following context:\n",
      "Big Thought's favorite color is blue.\n",
      "\n",
      "\n",
      "The human asks: \"What is Big Thought's favorite color?\"\n",
      "\n",
      "Your answer: Blue\n"
     ]
    }
   ],
   "source": [
    "print(qwen(prompt.format(question=\"What is Big Thought's favorite color?\").strip())[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, qwen manages to answer the question but it has troubles adhering to the rest of the prompt. \n",
    "It is often helpful to format your prompts using JSON-, XML-, or markdown-inspired formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "\"\"\"\n",
    "<INSTRUCTION>\n",
    "You are a helpful bot and are answering all questions the human has. \n",
    "You only answer the question and do not provide any additional information. \n",
    "You are not allowed to ask questions.\n",
    "</INSTRUCTION>\n",
    "\n",
    "<CONTEXT>\n",
    "Big Thought's favorite color is blue.\n",
    "</CONTEXT>\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<ANSWER>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<INSTRUCTION>\n",
      "You are a helpful bot and are answering all questions the human has. \n",
      "You only answer the question and do not provide any additional information. \n",
      "You are not allowed to ask questions.\n",
      "</INSTRUCTION>\n",
      "\n",
      "<CONTEXT>\n",
      "Big Thought's favorite color is blue.\n",
      "</CONTEXT>\n",
      "\n",
      "<QUESTION>\n",
      "What is Big Thought's favorite color?\n",
      "</QUESTION>\n",
      "\n",
      "<ANSWER>Blue</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "print(qwen(prompt.format(question=\"What is Big Thought's favorite color?\").strip())[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This not only helps the model, it also makes it easy for you to parse the output!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: Prompt engineering a simple chatbot\n",
    "\n",
    "With everything you've learnt so far, you are able to build a simple chatbot.\n",
    "Use the following prompt \"style\":\n",
    "Update the prompt with the previous in- and output every time you query the LLM!\n",
    "\n",
    "_Hint: Hugging Face `transformers` provides you with [helpful utilities](https://huggingface.co/docs/transformers/main/chat_templating) for prompt engineering chat models!_\n",
    "\n",
    "We provide you with a simple ipywidgets-based UI.\n",
    "\n",
    "_Can you prevent Qwen from hallucinating whole conversations?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e047d54a1f94834946505e72ea46853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', disabled=True), HBox(children=(Text(value='', placeholder='Type your message‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# TODO: Add your system prompt here. Instruct the LLM to generate a response to a given message.\n",
    "prompt = \\\n",
    "\"\"\"\n",
    "<INSTRUCTION>\n",
    "You are a friendly chatbot who always responds in the style of a pirate.\n",
    "You are not allowed to ask questions.\n",
    "You do not hallucinate or provide false information.\n",
    "<\\INSTRUCTION>\n",
    "\"\"\"\n",
    "\n",
    "def process_message(message: str, prompt: str):\n",
    "    # TODO: Add the message to the prompt and call the LLM to generate a response.\n",
    "    prompt = prompt + f\"<HUMAN>\\n{message}\\n</HUMAN>\\n\\n<BOT>\\n\"\n",
    "    response = qwen(prompt)[0]['generated_text']\n",
    "    response = response.split(\"<BOT>\")[1].strip().split(\"</BOT>\")[0]\n",
    "    return response\n",
    "\n",
    "user_input = widgets.Text(placeholder=\"Type your message here\")\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "chat_area = widgets.Textarea(disabled=True)\n",
    "\n",
    "\n",
    "def send_message(button):\n",
    "    global prompt\n",
    "    message = user_input.value\n",
    "    response = process_message(message, prompt)\n",
    "    chat_area.value = response\n",
    "    user_input.value = \"\"\n",
    "    prompt = response\n",
    "\n",
    "\n",
    "send_button.on_click(send_message)\n",
    "\n",
    "layout = widgets.VBox([chat_area, widgets.HBox([user_input, send_button])])\n",
    "\n",
    "display(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in LLMs and Prompt Engineering, we highly recommend you to take a closer look at [`langchain`](https://www.langchain.com) and [LlamaIndex](https://www.llamaindex.ai/)!\n",
    "\n",
    "----\n",
    "\n",
    "Now that you have some experience with prompt engineering, it's time to wrap our chatbot (which I assume is working flawlessly at this point ;P) in a more appealing UI. Go back to the Lab02 README to learn about streamlit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-lab-02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
